{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install all the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (1.11.0+cu113)\n",
      "Requirement already satisfied: torchvision in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (0.12.0+cu113)\n",
      "Requirement already satisfied: torchaudio in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (0.11.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: requests in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: numpy in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from torchvision) (1.22.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (1.22.3)\n",
      "Requirement already satisfied: sklearn in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from scikit-learn->sklearn) (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: transformers in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (4.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: filelock in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from transformers) (4.63.1)\n",
      "Requirement already satisfied: requests in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: click in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from sacremoses->transformers) (8.1.2)\n",
      "Requirement already satisfied: joblib in e:\\anaconda\\envs\\gpu2\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "%pip install numpy sklearn\n",
    "%pip install pandas transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# change directory to the directory of this script\n",
    "os.chdir('G:\\Projects\\CS-541-optional-project\\DAG_ERC_CODE')\n",
    "import DAG_ERC_CODE.dataloader as dataloader\n",
    "import DAG_ERC_CODE.dataset as dataset\n",
    "speaker_vocab, label_vocab, person_vec = dataloader.load_vocab('IEMOCAP')\n",
    "testset = dataset.IEMOCAPDataset('IEMOCAP', 'test',  speaker_vocab, label_vocab, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(bert_model_dir='', bert_tokenizer_dir='', bert_dim=1024, hidden_dim=300, mlp_layers=2, gnn_layers=4, emb_dim=1024, attn_type='rgcn', no_rel_attn=False, max_sent_len=200, no_cuda=False, dataset_name='IEMOCAP', windowp=1, windowf=0, max_grad_norm=5.0, lr=0.0005, dropout=0.2, batch_size=16, epochs=30, tensorboard=False, nodal_att_type=None)\n",
      "Running on GPU\n",
      "building vocab.. \n",
      "building datasets..\n",
      "100\n",
      "20\n",
      "31\n",
      "building model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-04-17 09:36:23,249][run.py][line:113][INFO] start training on GPU 0!\n",
      "[2022-04-17 09:36:23,250][run.py][line:114][INFO] Namespace(bert_model_dir='', bert_tokenizer_dir='', bert_dim=1024, hidden_dim=300, mlp_layers=2, gnn_layers=4, emb_dim=1024, attn_type='rgcn', no_rel_attn=False, max_sent_len=200, no_cuda=False, dataset_name='IEMOCAP', windowp=1, windowf=0, max_grad_norm=5.0, lr=0.0005, dropout=0.2, batch_size=16, epochs=30, tensorboard=False, nodal_att_type=None, cuda=True)\n",
      "E:\\Anaconda\\envs\\gpu2\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2022-04-17 09:37:01,406][run.py][line:168][INFO] Epoch: 1, train_loss: 1.2607, train_acc: 59.36, train_fscore: 58.84, valid_loss: 0.9525, valid_acc: 61.63, valid_fscore: 60.6, test_loss: 1.0045, test_acc: 63.32, test_fscore: 61.31, time: 29.61 sec\n",
      "[2022-04-17 09:37:17,825][run.py][line:168][INFO] Epoch: 2, train_loss: 0.6548, train_acc: 78.76, train_fscore: 78.57, valid_loss: 1.2267, valid_acc: 62.14, valid_fscore: 62.78, test_loss: 1.2387, test_acc: 62.64, test_fscore: 62.71, time: 16.42 sec\n",
      "[2022-04-17 09:37:34,882][run.py][line:168][INFO] Epoch: 3, train_loss: 0.5972, train_acc: 80.35, train_fscore: 80.36, valid_loss: 1.0997, valid_acc: 62.96, valid_fscore: 63.26, test_loss: 1.0174, test_acc: 64.73, test_fscore: 64.39, time: 17.06 sec\n",
      "[2022-04-17 09:37:51,458][run.py][line:168][INFO] Epoch: 4, train_loss: 0.5548, train_acc: 80.01, train_fscore: 79.7, valid_loss: 1.0581, valid_acc: 65.51, valid_fscore: 65.77, test_loss: 0.9439, test_acc: 64.12, test_fscore: 64.29, time: 16.58 sec\n",
      "[2022-04-17 09:38:08,166][run.py][line:168][INFO] Epoch: 5, train_loss: 0.525, train_acc: 81.98, train_fscore: 81.99, valid_loss: 1.0872, valid_acc: 63.98, valid_fscore: 64.35, test_loss: 0.98, test_acc: 66.21, test_fscore: 65.8, time: 16.71 sec\n",
      "[2022-04-17 09:38:24,863][run.py][line:168][INFO] Epoch: 6, train_loss: 0.5315, train_acc: 82.02, train_fscore: 81.97, valid_loss: 1.0903, valid_acc: 65.31, valid_fscore: 65.68, test_loss: 0.9779, test_acc: 65.23, test_fscore: 65.33, time: 16.7 sec\n",
      "[2022-04-17 09:38:41,660][run.py][line:168][INFO] Epoch: 7, train_loss: 0.5205, train_acc: 82.13, train_fscore: 82.08, valid_loss: 0.9907, valid_acc: 64.59, valid_fscore: 64.93, test_loss: 0.974, test_acc: 64.86, test_fscore: 64.66, time: 16.8 sec\n",
      "[2022-04-17 09:38:58,129][run.py][line:168][INFO] Epoch: 8, train_loss: 0.5043, train_acc: 83.55, train_fscore: 83.55, valid_loss: 1.0147, valid_acc: 64.49, valid_fscore: 64.88, test_loss: 0.9627, test_acc: 67.14, test_fscore: 67.12, time: 16.47 sec\n",
      "[2022-04-17 09:39:14,232][run.py][line:168][INFO] Epoch: 9, train_loss: 0.4864, train_acc: 83.19, train_fscore: 83.15, valid_loss: 0.9573, valid_acc: 65.1, valid_fscore: 65.39, test_loss: 0.9586, test_acc: 65.72, test_fscore: 65.66, time: 16.1 sec\n",
      "[2022-04-17 09:39:30,563][run.py][line:168][INFO] Epoch: 10, train_loss: 0.4609, train_acc: 83.8, train_fscore: 83.78, valid_loss: 1.0675, valid_acc: 64.9, valid_fscore: 65.45, test_loss: 0.9595, test_acc: 66.52, test_fscore: 66.54, time: 16.33 sec\n",
      "[2022-04-17 09:39:46,363][run.py][line:168][INFO] Epoch: 11, train_loss: 0.4627, train_acc: 83.8, train_fscore: 83.8, valid_loss: 1.0288, valid_acc: 63.57, valid_fscore: 64.14, test_loss: 0.9917, test_acc: 66.83, test_fscore: 66.67, time: 15.8 sec\n",
      "[2022-04-17 09:40:02,429][run.py][line:168][INFO] Epoch: 12, train_loss: 0.4673, train_acc: 83.24, train_fscore: 83.21, valid_loss: 1.0671, valid_acc: 64.69, valid_fscore: 65.18, test_loss: 0.9899, test_acc: 67.02, test_fscore: 67.03, time: 16.07 sec\n",
      "[2022-04-17 09:40:18,852][run.py][line:168][INFO] Epoch: 13, train_loss: 0.4695, train_acc: 84.07, train_fscore: 84.06, valid_loss: 1.0174, valid_acc: 65.41, valid_fscore: 65.72, test_loss: 0.9636, test_acc: 66.77, test_fscore: 66.6, time: 16.42 sec\n",
      "[2022-04-17 09:40:35,131][run.py][line:168][INFO] Epoch: 14, train_loss: 0.4655, train_acc: 84.64, train_fscore: 84.62, valid_loss: 1.1189, valid_acc: 63.47, valid_fscore: 64.01, test_loss: 0.9963, test_acc: 66.52, test_fscore: 66.47, time: 16.28 sec\n",
      "[2022-04-17 09:40:51,126][run.py][line:168][INFO] Epoch: 15, train_loss: 0.4395, train_acc: 84.55, train_fscore: 84.5, valid_loss: 1.1424, valid_acc: 66.22, valid_fscore: 66.7, test_loss: 1.0412, test_acc: 66.21, test_fscore: 66.26, time: 15.99 sec\n",
      "[2022-04-17 09:41:07,184][run.py][line:168][INFO] Epoch: 16, train_loss: 0.4609, train_acc: 84.47, train_fscore: 84.47, valid_loss: 1.1689, valid_acc: 64.49, valid_fscore: 64.93, test_loss: 0.9836, test_acc: 67.69, test_fscore: 67.78, time: 16.06 sec\n",
      "[2022-04-17 09:41:22,883][run.py][line:168][INFO] Epoch: 17, train_loss: 0.451, train_acc: 84.51, train_fscore: 84.48, valid_loss: 0.9934, valid_acc: 64.49, valid_fscore: 65.29, test_loss: 0.9687, test_acc: 66.71, test_fscore: 66.78, time: 15.7 sec\n",
      "[2022-04-17 09:41:38,820][run.py][line:168][INFO] Epoch: 18, train_loss: 0.4225, train_acc: 84.72, train_fscore: 84.75, valid_loss: 1.1178, valid_acc: 65.51, valid_fscore: 65.8, test_loss: 0.9982, test_acc: 66.77, test_fscore: 66.57, time: 15.94 sec\n",
      "[2022-04-17 09:41:54,728][run.py][line:168][INFO] Epoch: 19, train_loss: 0.4154, train_acc: 85.48, train_fscore: 85.43, valid_loss: 1.3259, valid_acc: 65.61, valid_fscore: 66.09, test_loss: 1.0322, test_acc: 66.71, test_fscore: 66.77, time: 15.91 sec\n",
      "[2022-04-17 09:42:10,781][run.py][line:168][INFO] Epoch: 20, train_loss: 0.4038, train_acc: 85.62, train_fscore: 85.65, valid_loss: 1.1141, valid_acc: 64.18, valid_fscore: 64.78, test_loss: 1.0354, test_acc: 67.51, test_fscore: 67.29, time: 16.05 sec\n",
      "[2022-04-17 09:42:26,603][run.py][line:168][INFO] Epoch: 21, train_loss: 0.3993, train_acc: 86.33, train_fscore: 86.29, valid_loss: 1.1293, valid_acc: 65.71, valid_fscore: 66.16, test_loss: 1.061, test_acc: 66.83, test_fscore: 66.88, time: 15.82 sec\n",
      "[2022-04-17 09:42:42,603][run.py][line:168][INFO] Epoch: 22, train_loss: 0.3888, train_acc: 86.5, train_fscore: 86.52, valid_loss: 1.0948, valid_acc: 64.49, valid_fscore: 64.93, test_loss: 1.0365, test_acc: 67.39, test_fscore: 67.13, time: 16.0 sec\n",
      "[2022-04-17 09:42:58,935][run.py][line:168][INFO] Epoch: 23, train_loss: 0.3915, train_acc: 87.13, train_fscore: 87.08, valid_loss: 1.2648, valid_acc: 61.73, valid_fscore: 62.64, test_loss: 1.1453, test_acc: 65.78, test_fscore: 65.83, time: 16.33 sec\n",
      "[2022-04-17 09:43:15,313][run.py][line:168][INFO] Epoch: 24, train_loss: 0.3681, train_acc: 85.73, train_fscore: 85.79, valid_loss: 1.2567, valid_acc: 63.37, valid_fscore: 63.81, test_loss: 1.11, test_acc: 66.46, test_fscore: 65.86, time: 16.38 sec\n",
      "[2022-04-17 09:43:31,357][run.py][line:168][INFO] Epoch: 25, train_loss: 0.3407, train_acc: 87.59, train_fscore: 87.57, valid_loss: 1.2674, valid_acc: 62.65, valid_fscore: 63.27, test_loss: 1.1843, test_acc: 66.89, test_fscore: 66.82, time: 16.04 sec\n",
      "[2022-04-17 09:43:46,972][run.py][line:168][INFO] Epoch: 26, train_loss: 0.3282, train_acc: 88.05, train_fscore: 88.04, valid_loss: 1.2001, valid_acc: 64.59, valid_fscore: 64.94, test_loss: 1.1506, test_acc: 66.09, test_fscore: 65.9, time: 15.62 sec\n",
      "[2022-04-17 09:44:02,742][run.py][line:168][INFO] Epoch: 27, train_loss: 0.3276, train_acc: 88.61, train_fscore: 88.6, valid_loss: 1.3315, valid_acc: 64.9, valid_fscore: 65.12, test_loss: 1.1711, test_acc: 66.46, test_fscore: 66.43, time: 15.77 sec\n",
      "[2022-04-17 09:44:18,551][run.py][line:168][INFO] Epoch: 28, train_loss: 0.3381, train_acc: 88.2, train_fscore: 88.19, valid_loss: 1.2861, valid_acc: 63.78, valid_fscore: 64.34, test_loss: 1.2127, test_acc: 66.15, test_fscore: 66.22, time: 15.81 sec\n",
      "[2022-04-17 09:44:34,740][run.py][line:168][INFO] Epoch: 29, train_loss: 0.306, train_acc: 88.32, train_fscore: 88.33, valid_loss: 1.5331, valid_acc: 62.65, valid_fscore: 63.03, test_loss: 1.1859, test_acc: 67.57, test_fscore: 67.34, time: 16.19 sec\n",
      "[2022-04-17 09:44:50,764][run.py][line:168][INFO] Epoch: 30, train_loss: 0.2868, train_acc: 89.31, train_fscore: 89.29, valid_loss: 1.3752, valid_acc: 63.57, valid_fscore: 63.93, test_loss: 1.2409, test_acc: 67.26, test_fscore: 67.18, time: 16.02 sec\n",
      "[2022-04-17 09:44:50,764][run.py][line:180][INFO] finish training!\n",
      "[2022-04-17 09:44:50,764][run.py][line:196][INFO] Best F-Score based on validation:66.26\n",
      "[2022-04-17 09:44:50,764][run.py][line:197][INFO] Best F-Score based on test:67.78\n"
     ]
    }
   ],
   "source": [
    "!python run.py --dataset IEMOCAP --gnn_layers 4 --lr 0.0005 --batch_size 16 --epochs 30 --dropout 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(bert_model_dir='', bert_tokenizer_dir='', bert_dim=1024, hidden_dim=300, mlp_layers=2, gnn_layers=2, emb_dim=1024, attn_type='rgcn', no_rel_attn=False, max_sent_len=200, no_cuda=False, dataset_name='MELD', windowp=1, windowf=0, max_grad_norm=5.0, lr=1e-05, dropout=0.1, batch_size=64, epochs=70, tensorboard=False, nodal_att_type=None)\n",
      "Running on GPU\n",
      "building vocab.. \n",
      "building datasets..\n",
      "1038\n",
      "114\n",
      "280\n",
      "building model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-04 02:23:15,738][run.py][line:113][INFO] start training on GPU 0!\n",
      "[2022-05-04 02:23:15,738][run.py][line:114][INFO] Namespace(bert_model_dir='', bert_tokenizer_dir='', bert_dim=1024, hidden_dim=300, mlp_layers=2, gnn_layers=2, emb_dim=1024, attn_type='rgcn', no_rel_attn=False, max_sent_len=200, no_cuda=False, dataset_name='MELD', windowp=1, windowf=0, max_grad_norm=5.0, lr=1e-05, dropout=0.1, batch_size=64, epochs=70, tensorboard=False, nodal_att_type=None, cuda=True)\n",
      "E:\\Anaconda\\envs\\gpu2\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2022-05-04 02:23:46,153][run.py][line:168][INFO] Epoch: 1, train_loss: 1.9108, train_acc: 30.21, train_fscore: 23.25, valid_loss: 1.8878, valid_acc: 46.89, valid_fscore: 45.14, test_loss: 1.8874, test_acc: 50.38, test_fscore: 48.42, time: 18.36 sec\n",
      "[2022-05-04 02:23:51,494][run.py][line:168][INFO] Epoch: 2, train_loss: 1.7585, train_acc: 80.32, train_fscore: 77.43, valid_loss: 1.7965, valid_acc: 57.53, valid_fscore: 56.84, test_loss: 1.7877, test_acc: 59.12, test_fscore: 58.26, time: 5.34 sec\n",
      "[2022-05-04 02:23:56,316][run.py][line:168][INFO] Epoch: 3, train_loss: 1.6008, train_acc: 89.71, train_fscore: 88.21, valid_loss: 1.6905, valid_acc: 61.23, valid_fscore: 59.64, test_loss: 1.6729, test_acc: 62.26, test_fscore: 60.8, time: 4.82 sec\n",
      "[2022-05-04 02:24:01,124][run.py][line:168][INFO] Epoch: 4, train_loss: 1.4183, train_acc: 91.9, train_fscore: 90.54, valid_loss: 1.568, valid_acc: 62.13, valid_fscore: 60.26, test_loss: 1.5383, test_acc: 63.49, test_fscore: 61.73, time: 4.81 sec\n",
      "[2022-05-04 02:24:06,001][run.py][line:168][INFO] Epoch: 5, train_loss: 1.2022, train_acc: 92.63, train_fscore: 91.42, valid_loss: 1.4415, valid_acc: 62.58, valid_fscore: 60.33, test_loss: 1.3977, test_acc: 63.72, test_fscore: 61.78, time: 4.88 sec\n",
      "[2022-05-04 02:24:10,878][run.py][line:168][INFO] Epoch: 6, train_loss: 0.9815, train_acc: 92.86, train_fscore: 91.71, valid_loss: 1.3349, valid_acc: 62.4, valid_fscore: 59.93, test_loss: 1.2794, test_acc: 63.87, test_fscore: 61.77, time: 4.88 sec\n",
      "[2022-05-04 02:24:15,646][run.py][line:168][INFO] Epoch: 7, train_loss: 0.7891, train_acc: 92.71, train_fscore: 91.59, valid_loss: 1.2678, valid_acc: 62.85, valid_fscore: 60.28, test_loss: 1.2119, test_acc: 63.75, test_fscore: 61.56, time: 4.77 sec\n",
      "[2022-05-04 02:24:20,586][run.py][line:168][INFO] Epoch: 8, train_loss: 0.6468, train_acc: 92.86, train_fscore: 91.85, valid_loss: 1.2498, valid_acc: 63.03, valid_fscore: 60.58, test_loss: 1.1886, test_acc: 63.83, test_fscore: 61.69, time: 4.94 sec\n",
      "[2022-05-04 02:24:25,605][run.py][line:168][INFO] Epoch: 9, train_loss: 0.5313, train_acc: 92.96, train_fscore: 91.95, valid_loss: 1.2293, valid_acc: 63.03, valid_fscore: 60.75, test_loss: 1.1876, test_acc: 63.95, test_fscore: 62.03, time: 5.02 sec\n",
      "[2022-05-04 02:24:30,372][run.py][line:168][INFO] Epoch: 10, train_loss: 0.4473, train_acc: 93.28, train_fscore: 92.43, valid_loss: 1.2285, valid_acc: 62.85, valid_fscore: 60.66, test_loss: 1.2029, test_acc: 64.14, test_fscore: 62.36, time: 4.77 sec\n",
      "[2022-05-04 02:24:34,991][run.py][line:168][INFO] Epoch: 11, train_loss: 0.3723, train_acc: 93.63, train_fscore: 92.92, valid_loss: 1.2364, valid_acc: 63.03, valid_fscore: 60.97, test_loss: 1.229, test_acc: 64.25, test_fscore: 62.66, time: 4.62 sec\n",
      "[2022-05-04 02:24:39,590][run.py][line:168][INFO] Epoch: 12, train_loss: 0.3255, train_acc: 94.17, train_fscore: 93.64, valid_loss: 1.2582, valid_acc: 63.12, valid_fscore: 61.32, test_loss: 1.265, test_acc: 64.21, test_fscore: 62.87, time: 4.6 sec\n",
      "[2022-05-04 02:24:45,099][run.py][line:168][INFO] Epoch: 13, train_loss: 0.2848, train_acc: 94.76, train_fscore: 94.51, valid_loss: 1.2993, valid_acc: 63.39, valid_fscore: 61.96, test_loss: 1.3043, test_acc: 64.18, test_fscore: 63.02, time: 5.51 sec\n",
      "[2022-05-04 02:24:49,756][run.py][line:168][INFO] Epoch: 14, train_loss: 0.2581, train_acc: 95.26, train_fscore: 95.14, valid_loss: 1.3108, valid_acc: 63.66, valid_fscore: 62.35, test_loss: 1.3446, test_acc: 63.83, test_fscore: 62.81, time: 4.66 sec\n",
      "[2022-05-04 02:24:54,478][run.py][line:168][INFO] Epoch: 15, train_loss: 0.2369, train_acc: 95.65, train_fscore: 95.59, valid_loss: 1.3357, valid_acc: 63.48, valid_fscore: 62.22, test_loss: 1.3806, test_acc: 64.02, test_fscore: 63.22, time: 4.72 sec\n",
      "[2022-05-04 02:24:59,153][run.py][line:168][INFO] Epoch: 16, train_loss: 0.2245, train_acc: 95.98, train_fscore: 95.95, valid_loss: 1.3723, valid_acc: 63.57, valid_fscore: 62.43, test_loss: 1.413, test_acc: 63.83, test_fscore: 63.1, time: 4.67 sec\n",
      "[2022-05-04 02:25:03,737][run.py][line:168][INFO] Epoch: 17, train_loss: 0.2076, train_acc: 96.05, train_fscore: 96.03, valid_loss: 1.3913, valid_acc: 63.39, valid_fscore: 62.3, test_loss: 1.4439, test_acc: 63.83, test_fscore: 63.15, time: 4.58 sec\n",
      "[2022-05-04 02:25:08,351][run.py][line:168][INFO] Epoch: 18, train_loss: 0.1987, train_acc: 96.12, train_fscore: 96.11, valid_loss: 1.4161, valid_acc: 63.48, valid_fscore: 62.54, test_loss: 1.4741, test_acc: 63.75, test_fscore: 63.14, time: 4.61 sec\n",
      "[2022-05-04 02:25:12,992][run.py][line:168][INFO] Epoch: 19, train_loss: 0.1897, train_acc: 96.06, train_fscore: 96.05, valid_loss: 1.4326, valid_acc: 63.48, valid_fscore: 62.58, test_loss: 1.5009, test_acc: 63.75, test_fscore: 63.21, time: 4.64 sec\n",
      "[2022-05-04 02:25:17,645][run.py][line:168][INFO] Epoch: 20, train_loss: 0.1796, train_acc: 96.19, train_fscore: 96.19, valid_loss: 1.461, valid_acc: 63.57, valid_fscore: 62.71, test_loss: 1.5257, test_acc: 63.75, test_fscore: 63.24, time: 4.65 sec\n",
      "[2022-05-04 02:25:22,222][run.py][line:168][INFO] Epoch: 21, train_loss: 0.174, train_acc: 96.09, train_fscore: 96.09, valid_loss: 1.4735, valid_acc: 63.48, valid_fscore: 62.62, test_loss: 1.5438, test_acc: 63.72, test_fscore: 63.21, time: 4.58 sec\n",
      "[2022-05-04 02:25:26,896][run.py][line:168][INFO] Epoch: 22, train_loss: 0.1697, train_acc: 96.12, train_fscore: 96.13, valid_loss: 1.4947, valid_acc: 63.39, valid_fscore: 62.6, test_loss: 1.563, test_acc: 63.56, test_fscore: 63.12, time: 4.67 sec\n",
      "[2022-05-04 02:25:32,333][run.py][line:168][INFO] Epoch: 23, train_loss: 0.1662, train_acc: 96.06, train_fscore: 96.07, valid_loss: 1.5084, valid_acc: 63.57, valid_fscore: 62.78, test_loss: 1.5801, test_acc: 63.64, test_fscore: 63.19, time: 5.44 sec\n",
      "[2022-05-04 02:25:37,887][run.py][line:168][INFO] Epoch: 24, train_loss: 0.1696, train_acc: 96.13, train_fscore: 96.14, valid_loss: 1.5224, valid_acc: 63.75, valid_fscore: 62.94, test_loss: 1.5933, test_acc: 63.75, test_fscore: 63.28, time: 5.55 sec\n",
      "[2022-05-04 02:25:43,250][run.py][line:168][INFO] Epoch: 25, train_loss: 0.1678, train_acc: 96.07, train_fscore: 96.08, valid_loss: 1.5191, valid_acc: 63.57, valid_fscore: 62.8, test_loss: 1.6021, test_acc: 63.56, test_fscore: 63.13, time: 5.36 sec\n",
      "[2022-05-04 02:25:48,683][run.py][line:168][INFO] Epoch: 26, train_loss: 0.1654, train_acc: 96.03, train_fscore: 96.04, valid_loss: 1.5328, valid_acc: 63.66, valid_fscore: 62.91, test_loss: 1.6063, test_acc: 63.6, test_fscore: 63.2, time: 5.43 sec\n",
      "[2022-05-04 02:25:54,077][run.py][line:168][INFO] Epoch: 27, train_loss: 0.1646, train_acc: 96.14, train_fscore: 96.15, valid_loss: 1.523, valid_acc: 63.84, valid_fscore: 63.1, test_loss: 1.618, test_acc: 63.64, test_fscore: 63.22, time: 5.39 sec\n",
      "[2022-05-04 02:25:59,529][run.py][line:168][INFO] Epoch: 28, train_loss: 0.1606, train_acc: 96.13, train_fscore: 96.14, valid_loss: 1.5444, valid_acc: 63.75, valid_fscore: 63.01, test_loss: 1.6203, test_acc: 63.83, test_fscore: 63.42, time: 5.45 sec\n",
      "[2022-05-04 02:26:04,114][run.py][line:168][INFO] Epoch: 29, train_loss: 0.1623, train_acc: 96.07, train_fscore: 96.08, valid_loss: 1.5484, valid_acc: 63.84, valid_fscore: 63.14, test_loss: 1.6299, test_acc: 63.64, test_fscore: 63.23, time: 4.58 sec\n",
      "[2022-05-04 02:26:09,513][run.py][line:168][INFO] Epoch: 30, train_loss: 0.1609, train_acc: 96.1, train_fscore: 96.11, valid_loss: 1.5573, valid_acc: 63.66, valid_fscore: 62.96, test_loss: 1.6314, test_acc: 63.6, test_fscore: 63.23, time: 5.4 sec\n",
      "[2022-05-04 02:26:14,876][run.py][line:168][INFO] Epoch: 31, train_loss: 0.1549, train_acc: 96.1, train_fscore: 96.11, valid_loss: 1.5586, valid_acc: 63.75, valid_fscore: 63.02, test_loss: 1.6391, test_acc: 63.64, test_fscore: 63.26, time: 5.36 sec\n",
      "[2022-05-04 02:26:20,310][run.py][line:168][INFO] Epoch: 32, train_loss: 0.1558, train_acc: 96.1, train_fscore: 96.11, valid_loss: 1.5713, valid_acc: 63.66, valid_fscore: 62.95, test_loss: 1.6414, test_acc: 63.87, test_fscore: 63.48, time: 5.43 sec\n",
      "[2022-05-04 02:26:26,048][run.py][line:168][INFO] Epoch: 33, train_loss: 0.157, train_acc: 96.12, train_fscore: 96.13, valid_loss: 1.5649, valid_acc: 63.66, valid_fscore: 62.95, test_loss: 1.6417, test_acc: 63.75, test_fscore: 63.36, time: 5.74 sec\n",
      "[2022-05-04 02:26:30,666][run.py][line:168][INFO] Epoch: 34, train_loss: 0.1613, train_acc: 96.08, train_fscore: 96.09, valid_loss: 1.5672, valid_acc: 63.75, valid_fscore: 63.06, test_loss: 1.643, test_acc: 63.72, test_fscore: 63.34, time: 4.62 sec\n",
      "[2022-05-04 02:26:36,105][run.py][line:168][INFO] Epoch: 35, train_loss: 0.158, train_acc: 96.16, train_fscore: 96.17, valid_loss: 1.5714, valid_acc: 63.57, valid_fscore: 62.87, test_loss: 1.6464, test_acc: 63.75, test_fscore: 63.36, time: 5.44 sec\n",
      "[2022-05-04 02:26:40,879][run.py][line:168][INFO] Epoch: 36, train_loss: 0.1535, train_acc: 96.13, train_fscore: 96.14, valid_loss: 1.5875, valid_acc: 63.75, valid_fscore: 63.0, test_loss: 1.6478, test_acc: 63.79, test_fscore: 63.4, time: 4.77 sec\n",
      "[2022-05-04 02:26:45,517][run.py][line:168][INFO] Epoch: 37, train_loss: 0.1558, train_acc: 96.19, train_fscore: 96.2, valid_loss: 1.5671, valid_acc: 63.75, valid_fscore: 62.99, test_loss: 1.6502, test_acc: 63.72, test_fscore: 63.33, time: 4.64 sec\n",
      "[2022-05-04 02:26:50,279][run.py][line:168][INFO] Epoch: 38, train_loss: 0.1574, train_acc: 96.1, train_fscore: 96.11, valid_loss: 1.5668, valid_acc: 63.66, valid_fscore: 62.95, test_loss: 1.6508, test_acc: 63.6, test_fscore: 63.25, time: 4.76 sec\n",
      "[2022-05-04 02:26:55,463][run.py][line:168][INFO] Epoch: 39, train_loss: 0.1522, train_acc: 96.11, train_fscore: 96.12, valid_loss: 1.5545, valid_acc: 63.84, valid_fscore: 63.1, test_loss: 1.6555, test_acc: 63.68, test_fscore: 63.26, time: 5.18 sec\n",
      "[2022-05-04 02:27:00,959][run.py][line:168][INFO] Epoch: 40, train_loss: 0.1587, train_acc: 96.09, train_fscore: 96.1, valid_loss: 1.5985, valid_acc: 63.84, valid_fscore: 63.1, test_loss: 1.6575, test_acc: 63.72, test_fscore: 63.34, time: 5.5 sec\n",
      "[2022-05-04 02:27:06,678][run.py][line:168][INFO] Epoch: 41, train_loss: 0.1564, train_acc: 96.09, train_fscore: 96.1, valid_loss: 1.5727, valid_acc: 63.48, valid_fscore: 62.73, test_loss: 1.6617, test_acc: 63.64, test_fscore: 63.27, time: 5.72 sec\n",
      "[2022-05-04 02:27:12,105][run.py][line:168][INFO] Epoch: 42, train_loss: 0.1579, train_acc: 96.19, train_fscore: 96.19, valid_loss: 1.5832, valid_acc: 64.02, valid_fscore: 63.31, test_loss: 1.655, test_acc: 63.83, test_fscore: 63.45, time: 5.43 sec\n",
      "[2022-05-04 02:27:18,085][run.py][line:168][INFO] Epoch: 43, train_loss: 0.1536, train_acc: 96.21, train_fscore: 96.22, valid_loss: 1.581, valid_acc: 63.66, valid_fscore: 62.95, test_loss: 1.658, test_acc: 63.45, test_fscore: 63.09, time: 5.98 sec\n",
      "[2022-05-04 02:27:24,342][run.py][line:168][INFO] Epoch: 44, train_loss: 0.151, train_acc: 96.1, train_fscore: 96.11, valid_loss: 1.5845, valid_acc: 63.75, valid_fscore: 62.99, test_loss: 1.6532, test_acc: 63.83, test_fscore: 63.47, time: 6.26 sec\n",
      "[2022-05-04 02:27:30,797][run.py][line:168][INFO] Epoch: 45, train_loss: 0.1525, train_acc: 96.18, train_fscore: 96.19, valid_loss: 1.5738, valid_acc: 63.57, valid_fscore: 62.83, test_loss: 1.6578, test_acc: 63.75, test_fscore: 63.42, time: 6.45 sec\n",
      "[2022-05-04 02:27:37,386][run.py][line:168][INFO] Epoch: 46, train_loss: 0.1492, train_acc: 96.19, train_fscore: 96.2, valid_loss: 1.5837, valid_acc: 63.66, valid_fscore: 62.91, test_loss: 1.662, test_acc: 63.75, test_fscore: 63.38, time: 6.59 sec\n",
      "[2022-05-04 02:27:43,850][run.py][line:168][INFO] Epoch: 47, train_loss: 0.1575, train_acc: 96.19, train_fscore: 96.2, valid_loss: 1.5817, valid_acc: 63.66, valid_fscore: 62.91, test_loss: 1.6669, test_acc: 63.83, test_fscore: 63.45, time: 6.46 sec\n",
      "[2022-05-04 02:27:50,715][run.py][line:168][INFO] Epoch: 48, train_loss: 0.1495, train_acc: 96.17, train_fscore: 96.17, valid_loss: 1.5719, valid_acc: 63.66, valid_fscore: 62.91, test_loss: 1.6566, test_acc: 63.79, test_fscore: 63.46, time: 6.86 sec\n",
      "[2022-05-04 02:27:56,996][run.py][line:168][INFO] Epoch: 49, train_loss: 0.1575, train_acc: 96.08, train_fscore: 96.09, valid_loss: 1.5631, valid_acc: 63.84, valid_fscore: 63.18, test_loss: 1.6557, test_acc: 63.83, test_fscore: 63.46, time: 6.28 sec\n",
      "[2022-05-04 02:28:02,571][run.py][line:168][INFO] Epoch: 50, train_loss: 0.1507, train_acc: 96.16, train_fscore: 96.17, valid_loss: 1.5831, valid_acc: 63.3, valid_fscore: 62.6, test_loss: 1.6645, test_acc: 63.6, test_fscore: 63.24, time: 5.57 sec\n",
      "[2022-05-04 02:28:08,953][run.py][line:168][INFO] Epoch: 51, train_loss: 0.1509, train_acc: 96.15, train_fscore: 96.16, valid_loss: 1.5388, valid_acc: 63.3, valid_fscore: 62.57, test_loss: 1.6619, test_acc: 63.68, test_fscore: 63.33, time: 6.38 sec\n",
      "[2022-05-04 02:28:14,319][run.py][line:168][INFO] Epoch: 52, train_loss: 0.155, train_acc: 96.22, train_fscore: 96.23, valid_loss: 1.5893, valid_acc: 63.57, valid_fscore: 62.83, test_loss: 1.6678, test_acc: 63.91, test_fscore: 63.53, time: 5.37 sec\n",
      "[2022-05-04 02:28:19,904][run.py][line:168][INFO] Epoch: 53, train_loss: 0.1507, train_acc: 96.16, train_fscore: 96.17, valid_loss: 1.5956, valid_acc: 63.66, valid_fscore: 62.94, test_loss: 1.6648, test_acc: 63.49, test_fscore: 63.12, time: 5.58 sec\n",
      "[2022-05-04 02:28:25,228][run.py][line:168][INFO] Epoch: 54, train_loss: 0.148, train_acc: 96.24, train_fscore: 96.25, valid_loss: 1.5845, valid_acc: 63.48, valid_fscore: 62.73, test_loss: 1.6615, test_acc: 63.83, test_fscore: 63.45, time: 5.32 sec\n",
      "[2022-05-04 02:28:30,704][run.py][line:168][INFO] Epoch: 55, train_loss: 0.153, train_acc: 96.19, train_fscore: 96.19, valid_loss: 1.5896, valid_acc: 63.3, valid_fscore: 62.56, test_loss: 1.667, test_acc: 63.68, test_fscore: 63.32, time: 5.48 sec\n",
      "[2022-05-04 02:28:36,144][run.py][line:168][INFO] Epoch: 56, train_loss: 0.1491, train_acc: 96.16, train_fscore: 96.17, valid_loss: 1.5709, valid_acc: 63.66, valid_fscore: 62.97, test_loss: 1.6568, test_acc: 64.02, test_fscore: 63.66, time: 5.44 sec\n",
      "[2022-05-04 02:28:41,533][run.py][line:168][INFO] Epoch: 57, train_loss: 0.1494, train_acc: 96.18, train_fscore: 96.19, valid_loss: 1.5831, valid_acc: 63.48, valid_fscore: 62.73, test_loss: 1.6631, test_acc: 63.79, test_fscore: 63.43, time: 5.39 sec\n",
      "[2022-05-04 02:28:46,809][run.py][line:168][INFO] Epoch: 58, train_loss: 0.1498, train_acc: 96.15, train_fscore: 96.16, valid_loss: 1.5795, valid_acc: 63.39, valid_fscore: 62.66, test_loss: 1.67, test_acc: 63.56, test_fscore: 63.2, time: 5.28 sec\n",
      "[2022-05-04 02:28:52,134][run.py][line:168][INFO] Epoch: 59, train_loss: 0.1502, train_acc: 96.22, train_fscore: 96.22, valid_loss: 1.5796, valid_acc: 63.12, valid_fscore: 62.4, test_loss: 1.6735, test_acc: 63.6, test_fscore: 63.24, time: 5.32 sec\n",
      "[2022-05-04 02:28:57,381][run.py][line:168][INFO] Epoch: 60, train_loss: 0.1553, train_acc: 96.2, train_fscore: 96.21, valid_loss: 1.5936, valid_acc: 63.12, valid_fscore: 62.38, test_loss: 1.6696, test_acc: 63.52, test_fscore: 63.19, time: 5.25 sec\n",
      "[2022-05-04 02:29:02,502][run.py][line:168][INFO] Epoch: 61, train_loss: 0.1487, train_acc: 96.19, train_fscore: 96.2, valid_loss: 1.5818, valid_acc: 63.75, valid_fscore: 63.05, test_loss: 1.6621, test_acc: 63.79, test_fscore: 63.45, time: 5.12 sec\n",
      "[2022-05-04 02:29:08,087][run.py][line:168][INFO] Epoch: 62, train_loss: 0.1484, train_acc: 96.22, train_fscore: 96.22, valid_loss: 1.5967, valid_acc: 63.39, valid_fscore: 62.65, test_loss: 1.677, test_acc: 63.6, test_fscore: 63.25, time: 5.59 sec\n",
      "[2022-05-04 02:29:13,414][run.py][line:168][INFO] Epoch: 63, train_loss: 0.1538, train_acc: 96.14, train_fscore: 96.15, valid_loss: 1.5812, valid_acc: 63.48, valid_fscore: 62.74, test_loss: 1.6688, test_acc: 63.79, test_fscore: 63.43, time: 5.33 sec\n",
      "[2022-05-04 02:29:18,872][run.py][line:168][INFO] Epoch: 64, train_loss: 0.144, train_acc: 96.22, train_fscore: 96.22, valid_loss: 1.5826, valid_acc: 63.3, valid_fscore: 62.55, test_loss: 1.6665, test_acc: 63.68, test_fscore: 63.3, time: 5.46 sec\n",
      "[2022-05-04 02:29:24,161][run.py][line:168][INFO] Epoch: 65, train_loss: 0.153, train_acc: 96.26, train_fscore: 96.26, valid_loss: 1.5966, valid_acc: 63.39, valid_fscore: 62.61, test_loss: 1.6763, test_acc: 63.68, test_fscore: 63.31, time: 5.29 sec\n",
      "[2022-05-04 02:29:29,595][run.py][line:168][INFO] Epoch: 66, train_loss: 0.1527, train_acc: 96.25, train_fscore: 96.26, valid_loss: 1.5854, valid_acc: 63.21, valid_fscore: 62.47, test_loss: 1.6726, test_acc: 63.72, test_fscore: 63.35, time: 5.43 sec\n",
      "[2022-05-04 02:29:35,040][run.py][line:168][INFO] Epoch: 67, train_loss: 0.1516, train_acc: 96.2, train_fscore: 96.21, valid_loss: 1.6017, valid_acc: 63.39, valid_fscore: 62.67, test_loss: 1.6764, test_acc: 63.52, test_fscore: 63.14, time: 5.44 sec\n",
      "[2022-05-04 02:29:40,567][run.py][line:168][INFO] Epoch: 68, train_loss: 0.1502, train_acc: 96.2, train_fscore: 96.21, valid_loss: 1.5816, valid_acc: 63.57, valid_fscore: 62.84, test_loss: 1.6669, test_acc: 63.72, test_fscore: 63.36, time: 5.53 sec\n",
      "[2022-05-04 02:29:46,439][run.py][line:168][INFO] Epoch: 69, train_loss: 0.1478, train_acc: 96.21, train_fscore: 96.22, valid_loss: 1.5792, valid_acc: 63.12, valid_fscore: 62.36, test_loss: 1.6717, test_acc: 63.52, test_fscore: 63.16, time: 5.87 sec\n",
      "[2022-05-04 02:29:52,111][run.py][line:168][INFO] Epoch: 70, train_loss: 0.1484, train_acc: 96.21, train_fscore: 96.22, valid_loss: 1.5998, valid_acc: 63.66, valid_fscore: 62.95, test_loss: 1.671, test_acc: 63.87, test_fscore: 63.48, time: 5.67 sec\n",
      "[2022-05-04 02:29:52,111][run.py][line:180][INFO] finish training!\n",
      "[2022-05-04 02:29:52,111][run.py][line:196][INFO] Best F-Score based on validation:63.45\n",
      "[2022-05-04 02:29:52,111][run.py][line:197][INFO] Best F-Score based on test:63.66\n"
     ]
    }
   ],
   "source": [
    "!python run.py --dataset MELD --lr 0.00001 --batch_size 64 --epochs 70 --dropout 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(bert_model_dir='', bert_tokenizer_dir='', bert_dim=1024, hidden_dim=300, mlp_layers=2, gnn_layers=2, emb_dim=1024, attn_type='rgcn', no_rel_attn=False, max_sent_len=200, no_cuda=False, dataset_name='EmoryNLP', windowp=1, windowf=0, max_grad_norm=5.0, lr=5e-05, dropout=0.3, batch_size=32, epochs=100, tensorboard=False, nodal_att_type=None)\n",
      "Running on GPU\n",
      "building vocab.. \n",
      "building datasets..\n",
      "713\n",
      "99\n",
      "85\n",
      "building model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-04 02:30:00,935][run.py][line:113][INFO] start training on GPU 0!\n",
      "[2022-05-04 02:30:00,935][run.py][line:114][INFO] Namespace(bert_model_dir='', bert_tokenizer_dir='', bert_dim=1024, hidden_dim=300, mlp_layers=2, gnn_layers=2, emb_dim=1024, attn_type='rgcn', no_rel_attn=False, max_sent_len=200, no_cuda=False, dataset_name='EmoryNLP', windowp=1, windowf=0, max_grad_norm=5.0, lr=5e-05, dropout=0.3, batch_size=32, epochs=100, tensorboard=False, nodal_att_type=None, cuda=True)\n",
      "E:\\Anaconda\\envs\\gpu2\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2022-05-04 02:30:16,493][run.py][line:168][INFO] Epoch: 1, train_loss: 1.7744, train_acc: 56.46, train_fscore: 52.86, valid_loss: 1.7114, valid_acc: 42.49, valid_fscore: 37.27, test_loss: 1.756, test_acc: 40.96, test_fscore: 34.78, time: 7.13 sec\n",
      "[2022-05-04 02:30:22,626][run.py][line:168][INFO] Epoch: 2, train_loss: 1.2427, train_acc: 68.77, train_fscore: 63.19, valid_loss: 1.6857, valid_acc: 42.49, valid_fscore: 38.46, test_loss: 1.7024, test_acc: 41.27, test_fscore: 36.23, time: 6.13 sec\n",
      "[2022-05-04 02:30:29,103][run.py][line:168][INFO] Epoch: 3, train_loss: 0.9322, train_acc: 72.31, train_fscore: 69.31, valid_loss: 1.708, valid_acc: 41.0, valid_fscore: 39.61, test_loss: 1.8224, test_acc: 39.83, test_fscore: 37.94, time: 6.48 sec\n",
      "[2022-05-04 02:30:35,163][run.py][line:168][INFO] Epoch: 4, train_loss: 0.872, train_acc: 73.05, train_fscore: 71.41, valid_loss: 1.8387, valid_acc: 40.4, valid_fscore: 39.28, test_loss: 1.8391, test_acc: 39.31, test_fscore: 37.7, time: 6.06 sec\n",
      "[2022-05-04 02:30:41,575][run.py][line:168][INFO] Epoch: 5, train_loss: 0.8635, train_acc: 73.58, train_fscore: 72.07, valid_loss: 1.9336, valid_acc: 40.25, valid_fscore: 39.06, test_loss: 1.8465, test_acc: 39.46, test_fscore: 38.01, time: 6.41 sec\n",
      "[2022-05-04 02:30:47,612][run.py][line:168][INFO] Epoch: 6, train_loss: 0.8355, train_acc: 73.62, train_fscore: 72.35, valid_loss: 1.8895, valid_acc: 40.03, valid_fscore: 38.99, test_loss: 1.8552, test_acc: 38.93, test_fscore: 37.64, time: 6.04 sec\n",
      "[2022-05-04 02:30:53,301][run.py][line:168][INFO] Epoch: 7, train_loss: 0.8432, train_acc: 73.99, train_fscore: 72.9, valid_loss: 1.9469, valid_acc: 39.81, valid_fscore: 38.88, test_loss: 1.8689, test_acc: 39.31, test_fscore: 37.89, time: 5.69 sec\n",
      "[2022-05-04 02:30:59,042][run.py][line:168][INFO] Epoch: 8, train_loss: 0.8511, train_acc: 74.1, train_fscore: 73.11, valid_loss: 1.9067, valid_acc: 40.77, valid_fscore: 39.7, test_loss: 1.8625, test_acc: 39.53, test_fscore: 37.98, time: 5.74 sec\n",
      "[2022-05-04 02:31:04,931][run.py][line:168][INFO] Epoch: 9, train_loss: 0.8372, train_acc: 74.19, train_fscore: 73.16, valid_loss: 1.8677, valid_acc: 39.66, valid_fscore: 39.17, test_loss: 1.8571, test_acc: 39.16, test_fscore: 37.96, time: 5.89 sec\n",
      "[2022-05-04 02:31:10,732][run.py][line:168][INFO] Epoch: 10, train_loss: 0.8403, train_acc: 73.74, train_fscore: 72.85, valid_loss: 1.9617, valid_acc: 40.48, valid_fscore: 39.5, test_loss: 1.8639, test_acc: 39.53, test_fscore: 38.16, time: 5.8 sec\n",
      "[2022-05-04 02:31:16,585][run.py][line:168][INFO] Epoch: 11, train_loss: 0.8312, train_acc: 74.34, train_fscore: 73.4, valid_loss: 1.9116, valid_acc: 39.66, valid_fscore: 38.83, test_loss: 1.8765, test_acc: 39.61, test_fscore: 38.29, time: 5.85 sec\n",
      "[2022-05-04 02:31:22,469][run.py][line:168][INFO] Epoch: 12, train_loss: 0.8196, train_acc: 74.31, train_fscore: 73.41, valid_loss: 1.8494, valid_acc: 39.73, valid_fscore: 38.9, test_loss: 1.8707, test_acc: 39.31, test_fscore: 38.09, time: 5.88 sec\n",
      "[2022-05-04 02:31:28,032][run.py][line:168][INFO] Epoch: 13, train_loss: 0.8233, train_acc: 74.61, train_fscore: 73.75, valid_loss: 1.8026, valid_acc: 39.73, valid_fscore: 39.01, test_loss: 1.8664, test_acc: 39.76, test_fscore: 38.55, time: 5.56 sec\n",
      "[2022-05-04 02:31:33,767][run.py][line:168][INFO] Epoch: 14, train_loss: 0.8315, train_acc: 74.33, train_fscore: 73.44, valid_loss: 1.8964, valid_acc: 40.18, valid_fscore: 39.32, test_loss: 1.8748, test_acc: 39.76, test_fscore: 38.45, time: 5.74 sec\n",
      "[2022-05-04 02:31:39,421][run.py][line:168][INFO] Epoch: 15, train_loss: 0.8126, train_acc: 74.59, train_fscore: 73.69, valid_loss: 1.7213, valid_acc: 39.88, valid_fscore: 38.97, test_loss: 1.8855, test_acc: 39.53, test_fscore: 38.17, time: 5.65 sec\n",
      "[2022-05-04 02:31:45,107][run.py][line:168][INFO] Epoch: 16, train_loss: 0.8116, train_acc: 74.73, train_fscore: 73.89, valid_loss: 1.9792, valid_acc: 40.03, valid_fscore: 39.32, test_loss: 1.876, test_acc: 39.46, test_fscore: 38.22, time: 5.69 sec\n",
      "[2022-05-04 02:31:50,755][run.py][line:168][INFO] Epoch: 17, train_loss: 0.8141, train_acc: 74.75, train_fscore: 73.81, valid_loss: 1.8482, valid_acc: 39.81, valid_fscore: 39.14, test_loss: 1.8821, test_acc: 39.46, test_fscore: 38.25, time: 5.65 sec\n",
      "[2022-05-04 02:31:56,595][run.py][line:168][INFO] Epoch: 18, train_loss: 0.8118, train_acc: 74.77, train_fscore: 73.88, valid_loss: 1.8748, valid_acc: 39.06, valid_fscore: 38.68, test_loss: 1.8795, test_acc: 39.46, test_fscore: 38.29, time: 5.84 sec\n",
      "[2022-05-04 02:32:02,360][run.py][line:168][INFO] Epoch: 19, train_loss: 0.8142, train_acc: 74.88, train_fscore: 74.1, valid_loss: 2.011, valid_acc: 39.73, valid_fscore: 39.03, test_loss: 1.8824, test_acc: 39.76, test_fscore: 38.54, time: 5.77 sec\n",
      "[2022-05-04 02:32:08,047][run.py][line:168][INFO] Epoch: 20, train_loss: 0.8092, train_acc: 74.75, train_fscore: 73.83, valid_loss: 1.9106, valid_acc: 38.76, valid_fscore: 38.31, test_loss: 1.8969, test_acc: 39.38, test_fscore: 38.24, time: 5.69 sec\n",
      "[2022-05-04 02:32:14,022][run.py][line:168][INFO] Epoch: 21, train_loss: 0.8031, train_acc: 75.06, train_fscore: 74.3, valid_loss: 1.9264, valid_acc: 39.66, valid_fscore: 39.21, test_loss: 1.897, test_acc: 39.53, test_fscore: 38.27, time: 5.97 sec\n",
      "[2022-05-04 02:32:19,762][run.py][line:168][INFO] Epoch: 22, train_loss: 0.8092, train_acc: 75.14, train_fscore: 74.38, valid_loss: 1.7654, valid_acc: 39.66, valid_fscore: 39.3, test_loss: 1.8786, test_acc: 39.23, test_fscore: 38.23, time: 5.74 sec\n",
      "[2022-05-04 02:32:25,529][run.py][line:168][INFO] Epoch: 23, train_loss: 0.8031, train_acc: 75.23, train_fscore: 74.43, valid_loss: 2.0449, valid_acc: 39.21, valid_fscore: 38.66, test_loss: 1.9021, test_acc: 39.38, test_fscore: 38.14, time: 5.77 sec\n",
      "[2022-05-04 02:32:32,189][run.py][line:168][INFO] Epoch: 24, train_loss: 0.8016, train_acc: 75.17, train_fscore: 74.38, valid_loss: 1.7781, valid_acc: 40.1, valid_fscore: 39.44, test_loss: 1.8962, test_acc: 39.53, test_fscore: 38.19, time: 6.66 sec\n",
      "[2022-05-04 02:32:38,129][run.py][line:168][INFO] Epoch: 25, train_loss: 0.7981, train_acc: 75.21, train_fscore: 74.45, valid_loss: 1.9922, valid_acc: 39.29, valid_fscore: 38.98, test_loss: 1.8989, test_acc: 39.08, test_fscore: 38.12, time: 5.94 sec\n",
      "[2022-05-04 02:32:43,830][run.py][line:168][INFO] Epoch: 26, train_loss: 0.7937, train_acc: 75.25, train_fscore: 74.52, valid_loss: 1.8731, valid_acc: 39.96, valid_fscore: 39.32, test_loss: 1.9139, test_acc: 39.53, test_fscore: 38.26, time: 5.7 sec\n",
      "[2022-05-04 02:32:49,520][run.py][line:168][INFO] Epoch: 27, train_loss: 0.7884, train_acc: 75.33, train_fscore: 74.55, valid_loss: 1.8948, valid_acc: 40.18, valid_fscore: 39.3, test_loss: 1.9133, test_acc: 39.91, test_fscore: 38.39, time: 5.69 sec\n",
      "[2022-05-04 02:32:55,072][run.py][line:168][INFO] Epoch: 28, train_loss: 0.7854, train_acc: 75.54, train_fscore: 74.85, valid_loss: 2.0339, valid_acc: 40.18, valid_fscore: 39.48, test_loss: 1.9345, test_acc: 39.83, test_fscore: 38.33, time: 5.55 sec\n",
      "[2022-05-04 02:33:00,715][run.py][line:168][INFO] Epoch: 29, train_loss: 0.7907, train_acc: 75.6, train_fscore: 74.84, valid_loss: 2.0408, valid_acc: 39.88, valid_fscore: 39.3, test_loss: 1.9252, test_acc: 39.91, test_fscore: 38.54, time: 5.64 sec\n",
      "[2022-05-04 02:33:06,370][run.py][line:168][INFO] Epoch: 30, train_loss: 0.7783, train_acc: 75.71, train_fscore: 75.05, valid_loss: 1.832, valid_acc: 39.58, valid_fscore: 38.79, test_loss: 1.9367, test_acc: 39.31, test_fscore: 37.85, time: 5.66 sec\n",
      "[2022-05-04 02:33:12,003][run.py][line:168][INFO] Epoch: 31, train_loss: 0.7774, train_acc: 75.82, train_fscore: 75.15, valid_loss: 1.9791, valid_acc: 40.4, valid_fscore: 39.37, test_loss: 1.9827, test_acc: 39.91, test_fscore: 38.23, time: 5.63 sec\n",
      "[2022-05-04 02:33:17,596][run.py][line:168][INFO] Epoch: 32, train_loss: 0.7706, train_acc: 75.86, train_fscore: 75.15, valid_loss: 1.7661, valid_acc: 39.51, valid_fscore: 39.14, test_loss: 1.92, test_acc: 39.68, test_fscore: 38.64, time: 5.59 sec\n",
      "[2022-05-04 02:33:23,245][run.py][line:168][INFO] Epoch: 33, train_loss: 0.7735, train_acc: 75.95, train_fscore: 75.31, valid_loss: 1.8268, valid_acc: 39.88, valid_fscore: 39.2, test_loss: 1.9415, test_acc: 40.14, test_fscore: 38.77, time: 5.65 sec\n",
      "[2022-05-04 02:33:28,920][run.py][line:168][INFO] Epoch: 34, train_loss: 0.7634, train_acc: 76.01, train_fscore: 75.37, valid_loss: 1.9998, valid_acc: 40.25, valid_fscore: 39.61, test_loss: 1.9527, test_acc: 40.29, test_fscore: 39.09, time: 5.67 sec\n",
      "[2022-05-04 02:33:34,698][run.py][line:168][INFO] Epoch: 35, train_loss: 0.7595, train_acc: 76.32, train_fscore: 75.72, valid_loss: 1.9187, valid_acc: 39.36, valid_fscore: 38.6, test_loss: 1.9749, test_acc: 40.29, test_fscore: 38.83, time: 5.78 sec\n",
      "[2022-05-04 02:33:40,270][run.py][line:168][INFO] Epoch: 36, train_loss: 0.7677, train_acc: 76.12, train_fscore: 75.46, valid_loss: 2.0783, valid_acc: 39.73, valid_fscore: 38.97, test_loss: 1.9765, test_acc: 39.76, test_fscore: 38.2, time: 5.57 sec\n",
      "[2022-05-04 02:33:45,893][run.py][line:168][INFO] Epoch: 37, train_loss: 0.7447, train_acc: 76.48, train_fscore: 75.82, valid_loss: 2.0396, valid_acc: 39.73, valid_fscore: 39.23, test_loss: 1.9649, test_acc: 39.76, test_fscore: 38.71, time: 5.62 sec\n",
      "[2022-05-04 02:33:51,607][run.py][line:168][INFO] Epoch: 38, train_loss: 0.751, train_acc: 76.5, train_fscore: 75.88, valid_loss: 2.0199, valid_acc: 39.29, valid_fscore: 38.85, test_loss: 1.969, test_acc: 39.46, test_fscore: 38.44, time: 5.71 sec\n",
      "[2022-05-04 02:33:57,363][run.py][line:168][INFO] Epoch: 39, train_loss: 0.7453, train_acc: 76.49, train_fscore: 75.95, valid_loss: 2.1232, valid_acc: 40.1, valid_fscore: 39.3, test_loss: 1.9891, test_acc: 39.98, test_fscore: 38.67, time: 5.75 sec\n",
      "[2022-05-04 02:34:03,009][run.py][line:168][INFO] Epoch: 40, train_loss: 0.7265, train_acc: 76.53, train_fscore: 75.9, valid_loss: 1.8038, valid_acc: 39.51, valid_fscore: 38.94, test_loss: 1.9984, test_acc: 40.06, test_fscore: 38.86, time: 5.65 sec\n",
      "[2022-05-04 02:34:08,624][run.py][line:168][INFO] Epoch: 41, train_loss: 0.7231, train_acc: 76.99, train_fscore: 76.48, valid_loss: 2.0494, valid_acc: 39.36, valid_fscore: 38.83, test_loss: 2.0326, test_acc: 40.14, test_fscore: 38.74, time: 5.61 sec\n",
      "[2022-05-04 02:34:14,227][run.py][line:168][INFO] Epoch: 42, train_loss: 0.7236, train_acc: 77.13, train_fscore: 76.56, valid_loss: 2.1167, valid_acc: 39.43, valid_fscore: 38.67, test_loss: 2.0437, test_acc: 40.21, test_fscore: 38.61, time: 5.6 sec\n",
      "[2022-05-04 02:34:20,122][run.py][line:168][INFO] Epoch: 43, train_loss: 0.7248, train_acc: 77.43, train_fscore: 76.93, valid_loss: 1.9949, valid_acc: 40.1, valid_fscore: 39.24, test_loss: 2.0, test_acc: 40.36, test_fscore: 38.76, time: 5.89 sec\n",
      "[2022-05-04 02:34:25,838][run.py][line:168][INFO] Epoch: 44, train_loss: 0.7058, train_acc: 77.35, train_fscore: 76.78, valid_loss: 2.0588, valid_acc: 38.62, valid_fscore: 38.16, test_loss: 2.0417, test_acc: 39.23, test_fscore: 38.12, time: 5.72 sec\n",
      "[2022-05-04 02:34:31,583][run.py][line:168][INFO] Epoch: 45, train_loss: 0.71, train_acc: 77.51, train_fscore: 76.97, valid_loss: 1.9774, valid_acc: 38.62, valid_fscore: 38.7, test_loss: 2.0396, test_acc: 39.01, test_fscore: 38.51, time: 5.74 sec\n",
      "[2022-05-04 02:34:37,097][run.py][line:168][INFO] Epoch: 46, train_loss: 0.6967, train_acc: 77.97, train_fscore: 77.47, valid_loss: 2.0575, valid_acc: 39.21, valid_fscore: 38.85, test_loss: 2.0517, test_acc: 39.38, test_fscore: 38.53, time: 5.51 sec\n",
      "[2022-05-04 02:34:42,781][run.py][line:168][INFO] Epoch: 47, train_loss: 0.6849, train_acc: 78.1, train_fscore: 77.61, valid_loss: 2.034, valid_acc: 38.69, valid_fscore: 38.23, test_loss: 2.0775, test_acc: 38.86, test_fscore: 37.93, time: 5.68 sec\n",
      "[2022-05-04 02:34:48,287][run.py][line:168][INFO] Epoch: 48, train_loss: 0.6748, train_acc: 78.46, train_fscore: 78.02, valid_loss: 2.1836, valid_acc: 38.69, valid_fscore: 38.16, test_loss: 2.0856, test_acc: 39.08, test_fscore: 37.92, time: 5.51 sec\n",
      "[2022-05-04 02:34:53,922][run.py][line:168][INFO] Epoch: 49, train_loss: 0.6666, train_acc: 78.52, train_fscore: 78.03, valid_loss: 2.053, valid_acc: 38.91, valid_fscore: 38.48, test_loss: 2.0709, test_acc: 39.01, test_fscore: 38.22, time: 5.63 sec\n",
      "[2022-05-04 02:34:59,485][run.py][line:168][INFO] Epoch: 50, train_loss: 0.6685, train_acc: 78.96, train_fscore: 78.55, valid_loss: 2.2267, valid_acc: 38.32, valid_fscore: 38.04, test_loss: 2.1189, test_acc: 38.93, test_fscore: 38.13, time: 5.56 sec\n",
      "[2022-05-04 02:35:05,128][run.py][line:168][INFO] Epoch: 51, train_loss: 0.6402, train_acc: 79.13, train_fscore: 78.67, valid_loss: 2.0771, valid_acc: 38.32, valid_fscore: 38.1, test_loss: 2.1485, test_acc: 38.93, test_fscore: 38.09, time: 5.64 sec\n",
      "[2022-05-04 02:35:10,644][run.py][line:168][INFO] Epoch: 52, train_loss: 0.6362, train_acc: 79.31, train_fscore: 78.94, valid_loss: 2.1512, valid_acc: 38.62, valid_fscore: 38.26, test_loss: 2.1635, test_acc: 38.7, test_fscore: 37.57, time: 5.52 sec\n",
      "[2022-05-04 02:35:16,344][run.py][line:168][INFO] Epoch: 53, train_loss: 0.6351, train_acc: 79.65, train_fscore: 79.23, valid_loss: 2.154, valid_acc: 37.05, valid_fscore: 36.96, test_loss: 2.1811, test_acc: 39.16, test_fscore: 38.27, time: 5.7 sec\n",
      "[2022-05-04 02:35:22,003][run.py][line:168][INFO] Epoch: 54, train_loss: 0.6238, train_acc: 79.76, train_fscore: 79.33, valid_loss: 2.2994, valid_acc: 38.39, valid_fscore: 38.28, test_loss: 2.2223, test_acc: 39.61, test_fscore: 38.95, time: 5.66 sec\n",
      "[2022-05-04 02:35:27,548][run.py][line:168][INFO] Epoch: 55, train_loss: 0.6036, train_acc: 80.76, train_fscore: 80.41, valid_loss: 2.4807, valid_acc: 37.8, valid_fscore: 36.99, test_loss: 2.2657, test_acc: 38.7, test_fscore: 37.36, time: 5.54 sec\n",
      "[2022-05-04 02:35:33,221][run.py][line:168][INFO] Epoch: 56, train_loss: 0.5867, train_acc: 81.05, train_fscore: 80.74, valid_loss: 2.4288, valid_acc: 38.17, valid_fscore: 37.64, test_loss: 2.2702, test_acc: 39.68, test_fscore: 38.6, time: 5.67 sec\n",
      "[2022-05-04 02:35:38,851][run.py][line:168][INFO] Epoch: 57, train_loss: 0.573, train_acc: 81.36, train_fscore: 81.04, valid_loss: 2.3695, valid_acc: 37.87, valid_fscore: 37.42, test_loss: 2.2935, test_acc: 39.16, test_fscore: 38.04, time: 5.63 sec\n",
      "[2022-05-04 02:35:44,382][run.py][line:168][INFO] Epoch: 58, train_loss: 0.557, train_acc: 82.06, train_fscore: 81.76, valid_loss: 2.3658, valid_acc: 37.43, valid_fscore: 37.45, test_loss: 2.3348, test_acc: 39.01, test_fscore: 38.53, time: 5.53 sec\n",
      "[2022-05-04 02:35:50,129][run.py][line:168][INFO] Epoch: 59, train_loss: 0.5388, train_acc: 82.53, train_fscore: 82.25, valid_loss: 2.4452, valid_acc: 37.65, valid_fscore: 37.29, test_loss: 2.3574, test_acc: 39.46, test_fscore: 38.48, time: 5.75 sec\n",
      "[2022-05-04 02:35:55,735][run.py][line:168][INFO] Epoch: 60, train_loss: 0.5209, train_acc: 83.36, train_fscore: 83.1, valid_loss: 2.4404, valid_acc: 36.9, valid_fscore: 36.94, test_loss: 2.4696, test_acc: 38.48, test_fscore: 37.68, time: 5.61 sec\n",
      "[2022-05-04 02:36:02,022][run.py][line:168][INFO] Epoch: 61, train_loss: 0.5074, train_acc: 83.59, train_fscore: 83.33, valid_loss: 2.6025, valid_acc: 37.65, valid_fscore: 37.28, test_loss: 2.5117, test_acc: 39.23, test_fscore: 38.37, time: 6.29 sec\n",
      "[2022-05-04 02:36:07,700][run.py][line:168][INFO] Epoch: 62, train_loss: 0.4923, train_acc: 84.56, train_fscore: 84.34, valid_loss: 2.8779, valid_acc: 36.53, valid_fscore: 36.4, test_loss: 2.5015, test_acc: 38.7, test_fscore: 38.1, time: 5.68 sec\n",
      "[2022-05-04 02:36:13,374][run.py][line:168][INFO] Epoch: 63, train_loss: 0.4782, train_acc: 84.96, train_fscore: 84.74, valid_loss: 2.5698, valid_acc: 37.43, valid_fscore: 37.15, test_loss: 2.5367, test_acc: 39.08, test_fscore: 38.35, time: 5.67 sec\n",
      "[2022-05-04 02:36:19,113][run.py][line:168][INFO] Epoch: 64, train_loss: 0.4761, train_acc: 84.83, train_fscore: 84.64, valid_loss: 2.7434, valid_acc: 36.9, valid_fscore: 36.63, test_loss: 2.5781, test_acc: 39.16, test_fscore: 38.23, time: 5.74 sec\n",
      "[2022-05-04 02:36:24,645][run.py][line:168][INFO] Epoch: 65, train_loss: 0.4449, train_acc: 85.88, train_fscore: 85.69, valid_loss: 2.7394, valid_acc: 36.61, valid_fscore: 36.95, test_loss: 2.6405, test_acc: 38.78, test_fscore: 38.63, time: 5.53 sec\n",
      "[2022-05-04 02:36:30,231][run.py][line:168][INFO] Epoch: 66, train_loss: 0.4223, train_acc: 87.02, train_fscore: 86.87, valid_loss: 2.8092, valid_acc: 37.28, valid_fscore: 37.07, test_loss: 2.6786, test_acc: 38.7, test_fscore: 37.94, time: 5.59 sec\n",
      "[2022-05-04 02:36:35,838][run.py][line:168][INFO] Epoch: 67, train_loss: 0.406, train_acc: 87.44, train_fscore: 87.29, valid_loss: 2.8321, valid_acc: 36.24, valid_fscore: 36.23, test_loss: 2.7271, test_acc: 38.7, test_fscore: 38.19, time: 5.61 sec\n",
      "[2022-05-04 02:36:41,499][run.py][line:168][INFO] Epoch: 68, train_loss: 0.3853, train_acc: 88.27, train_fscore: 88.14, valid_loss: 3.0623, valid_acc: 36.09, valid_fscore: 36.24, test_loss: 2.8264, test_acc: 39.31, test_fscore: 38.9, time: 5.66 sec\n",
      "[2022-05-04 02:36:47,121][run.py][line:168][INFO] Epoch: 69, train_loss: 0.3652, train_acc: 88.77, train_fscore: 88.66, valid_loss: 3.1658, valid_acc: 36.68, valid_fscore: 36.46, test_loss: 2.9072, test_acc: 38.4, test_fscore: 37.72, time: 5.62 sec\n",
      "[2022-05-04 02:36:52,699][run.py][line:168][INFO] Epoch: 70, train_loss: 0.3546, train_acc: 89.16, train_fscore: 89.08, valid_loss: 2.9811, valid_acc: 36.68, valid_fscore: 35.91, test_loss: 3.0226, test_acc: 39.38, test_fscore: 37.72, time: 5.58 sec\n",
      "[2022-05-04 02:36:58,462][run.py][line:168][INFO] Epoch: 71, train_loss: 0.3351, train_acc: 89.52, train_fscore: 89.42, valid_loss: 3.1374, valid_acc: 36.53, valid_fscore: 36.28, test_loss: 3.0822, test_acc: 38.78, test_fscore: 37.91, time: 5.76 sec\n",
      "[2022-05-04 02:37:04,262][run.py][line:168][INFO] Epoch: 72, train_loss: 0.3293, train_acc: 89.92, train_fscore: 89.83, valid_loss: 3.0817, valid_acc: 36.53, valid_fscore: 36.51, test_loss: 3.0748, test_acc: 38.63, test_fscore: 38.21, time: 5.8 sec\n",
      "[2022-05-04 02:37:09,959][run.py][line:168][INFO] Epoch: 73, train_loss: 0.304, train_acc: 90.98, train_fscore: 90.91, valid_loss: 3.6333, valid_acc: 35.86, valid_fscore: 36.29, test_loss: 3.2417, test_acc: 37.58, test_fscore: 37.38, time: 5.7 sec\n",
      "[2022-05-04 02:37:15,626][run.py][line:168][INFO] Epoch: 74, train_loss: 0.2986, train_acc: 91.03, train_fscore: 90.99, valid_loss: 3.4401, valid_acc: 35.86, valid_fscore: 35.93, test_loss: 3.3378, test_acc: 37.95, test_fscore: 37.53, time: 5.67 sec\n",
      "[2022-05-04 02:37:21,303][run.py][line:168][INFO] Epoch: 75, train_loss: 0.279, train_acc: 91.79, train_fscore: 91.73, valid_loss: 3.3281, valid_acc: 36.76, valid_fscore: 36.73, test_loss: 3.3205, test_acc: 38.7, test_fscore: 38.21, time: 5.68 sec\n",
      "[2022-05-04 02:37:26,832][run.py][line:168][INFO] Epoch: 76, train_loss: 0.2701, train_acc: 91.94, train_fscore: 91.88, valid_loss: 3.3503, valid_acc: 36.83, valid_fscore: 36.75, test_loss: 3.3981, test_acc: 38.33, test_fscore: 37.57, time: 5.53 sec\n",
      "[2022-05-04 02:37:32,412][run.py][line:168][INFO] Epoch: 77, train_loss: 0.2682, train_acc: 92.18, train_fscore: 92.13, valid_loss: 3.47, valid_acc: 35.64, valid_fscore: 35.85, test_loss: 3.5304, test_acc: 38.03, test_fscore: 37.42, time: 5.58 sec\n",
      "[2022-05-04 02:37:39,212][run.py][line:168][INFO] Epoch: 78, train_loss: 0.2573, train_acc: 92.51, train_fscore: 92.46, valid_loss: 3.7561, valid_acc: 35.57, valid_fscore: 35.26, test_loss: 3.6488, test_acc: 38.78, test_fscore: 37.62, time: 6.8 sec\n",
      "[2022-05-04 02:37:45,281][run.py][line:168][INFO] Epoch: 79, train_loss: 0.2394, train_acc: 92.82, train_fscore: 92.78, valid_loss: 3.7791, valid_acc: 36.16, valid_fscore: 36.15, test_loss: 3.6808, test_acc: 37.58, test_fscore: 36.91, time: 6.07 sec\n",
      "[2022-05-04 02:37:50,949][run.py][line:168][INFO] Epoch: 80, train_loss: 0.2352, train_acc: 93.35, train_fscore: 93.31, valid_loss: 4.1145, valid_acc: 35.94, valid_fscore: 35.94, test_loss: 3.7283, test_acc: 38.93, test_fscore: 38.33, time: 5.67 sec\n",
      "[2022-05-04 02:37:56,616][run.py][line:168][INFO] Epoch: 81, train_loss: 0.2238, train_acc: 93.37, train_fscore: 93.34, valid_loss: 3.8545, valid_acc: 36.83, valid_fscore: 36.66, test_loss: 3.7823, test_acc: 38.1, test_fscore: 37.22, time: 5.67 sec\n",
      "[2022-05-04 02:38:02,251][run.py][line:168][INFO] Epoch: 82, train_loss: 0.205, train_acc: 93.84, train_fscore: 93.81, valid_loss: 4.0277, valid_acc: 36.09, valid_fscore: 36.23, test_loss: 3.8694, test_acc: 38.55, test_fscore: 38.15, time: 5.64 sec\n",
      "[2022-05-04 02:38:07,828][run.py][line:168][INFO] Epoch: 83, train_loss: 0.1993, train_acc: 94.04, train_fscore: 94.01, valid_loss: 4.0256, valid_acc: 36.09, valid_fscore: 35.84, test_loss: 3.948, test_acc: 38.55, test_fscore: 37.71, time: 5.58 sec\n",
      "[2022-05-04 02:38:13,551][run.py][line:168][INFO] Epoch: 84, train_loss: 0.1886, train_acc: 94.72, train_fscore: 94.69, valid_loss: 3.9167, valid_acc: 35.27, valid_fscore: 34.87, test_loss: 4.0547, test_acc: 38.18, test_fscore: 36.99, time: 5.72 sec\n",
      "[2022-05-04 02:38:19,267][run.py][line:168][INFO] Epoch: 85, train_loss: 0.1841, train_acc: 94.62, train_fscore: 94.6, valid_loss: 4.3456, valid_acc: 35.71, valid_fscore: 35.69, test_loss: 4.0728, test_acc: 38.78, test_fscore: 38.1, time: 5.72 sec\n",
      "[2022-05-04 02:38:24,898][run.py][line:168][INFO] Epoch: 86, train_loss: 0.1746, train_acc: 94.92, train_fscore: 94.9, valid_loss: 4.7445, valid_acc: 35.79, valid_fscore: 36.16, test_loss: 4.1589, test_acc: 38.33, test_fscore: 37.97, time: 5.63 sec\n",
      "[2022-05-04 02:38:30,608][run.py][line:168][INFO] Epoch: 87, train_loss: 0.1707, train_acc: 95.1, train_fscore: 95.08, valid_loss: 4.9984, valid_acc: 36.31, valid_fscore: 36.57, test_loss: 4.2995, test_acc: 38.1, test_fscore: 37.47, time: 5.71 sec\n",
      "[2022-05-04 02:38:36,302][run.py][line:168][INFO] Epoch: 88, train_loss: 0.165, train_acc: 95.39, train_fscore: 95.37, valid_loss: 4.2079, valid_acc: 35.86, valid_fscore: 35.8, test_loss: 4.3609, test_acc: 38.25, test_fscore: 37.53, time: 5.69 sec\n",
      "[2022-05-04 02:38:41,887][run.py][line:168][INFO] Epoch: 89, train_loss: 0.163, train_acc: 95.07, train_fscore: 95.05, valid_loss: 3.8546, valid_acc: 36.16, valid_fscore: 36.02, test_loss: 4.4922, test_acc: 38.48, test_fscore: 37.65, time: 5.58 sec\n",
      "[2022-05-04 02:38:47,436][run.py][line:168][INFO] Epoch: 90, train_loss: 0.1548, train_acc: 95.39, train_fscore: 95.37, valid_loss: 4.8977, valid_acc: 37.35, valid_fscore: 37.23, test_loss: 4.6809, test_acc: 37.95, test_fscore: 36.93, time: 5.55 sec\n",
      "[2022-05-04 02:38:53,122][run.py][line:168][INFO] Epoch: 91, train_loss: 0.1444, train_acc: 95.83, train_fscore: 95.82, valid_loss: 4.6438, valid_acc: 35.27, valid_fscore: 35.36, test_loss: 4.5029, test_acc: 38.1, test_fscore: 37.45, time: 5.69 sec\n",
      "[2022-05-04 02:38:58,736][run.py][line:168][INFO] Epoch: 92, train_loss: 0.1444, train_acc: 95.59, train_fscore: 95.58, valid_loss: 5.1765, valid_acc: 36.01, valid_fscore: 36.01, test_loss: 4.7098, test_acc: 38.18, test_fscore: 37.42, time: 5.61 sec\n",
      "[2022-05-04 02:39:04,421][run.py][line:168][INFO] Epoch: 93, train_loss: 0.1346, train_acc: 96.1, train_fscore: 96.1, valid_loss: 4.8044, valid_acc: 35.57, valid_fscore: 35.79, test_loss: 4.7368, test_acc: 37.95, test_fscore: 37.21, time: 5.69 sec\n",
      "[2022-05-04 02:39:10,001][run.py][line:168][INFO] Epoch: 94, train_loss: 0.1333, train_acc: 96.1, train_fscore: 96.09, valid_loss: 5.4668, valid_acc: 35.27, valid_fscore: 35.41, test_loss: 4.8095, test_acc: 38.03, test_fscore: 37.39, time: 5.58 sec\n",
      "[2022-05-04 02:39:15,668][run.py][line:168][INFO] Epoch: 95, train_loss: 0.1229, train_acc: 96.51, train_fscore: 96.5, valid_loss: 5.2595, valid_acc: 36.61, valid_fscore: 36.44, test_loss: 4.9756, test_acc: 38.25, test_fscore: 37.36, time: 5.67 sec\n",
      "[2022-05-04 02:39:21,343][run.py][line:168][INFO] Epoch: 96, train_loss: 0.1334, train_acc: 96.07, train_fscore: 96.06, valid_loss: 4.7136, valid_acc: 35.34, valid_fscore: 35.52, test_loss: 4.8431, test_acc: 38.1, test_fscore: 37.59, time: 5.67 sec\n",
      "[2022-05-04 02:39:26,999][run.py][line:168][INFO] Epoch: 97, train_loss: 0.1365, train_acc: 95.87, train_fscore: 95.86, valid_loss: 4.6675, valid_acc: 35.64, valid_fscore: 35.63, test_loss: 4.8599, test_acc: 38.03, test_fscore: 37.38, time: 5.66 sec\n",
      "[2022-05-04 02:39:32,544][run.py][line:168][INFO] Epoch: 98, train_loss: 0.1332, train_acc: 96.1, train_fscore: 96.09, valid_loss: 4.9357, valid_acc: 35.12, valid_fscore: 35.11, test_loss: 4.9552, test_acc: 38.18, test_fscore: 37.56, time: 5.54 sec\n",
      "[2022-05-04 02:39:38,114][run.py][line:168][INFO] Epoch: 99, train_loss: 0.1297, train_acc: 96.18, train_fscore: 96.18, valid_loss: 5.1327, valid_acc: 36.01, valid_fscore: 35.84, test_loss: 4.994, test_acc: 37.35, test_fscore: 36.34, time: 5.57 sec\n",
      "[2022-05-04 02:39:43,710][run.py][line:168][INFO] Epoch: 100, train_loss: 0.1224, train_acc: 96.41, train_fscore: 96.4, valid_loss: 5.4607, valid_acc: 35.71, valid_fscore: 35.76, test_loss: 5.0451, test_acc: 38.03, test_fscore: 37.35, time: 5.6 sec\n",
      "[2022-05-04 02:39:43,710][run.py][line:180][INFO] finish training!\n",
      "[2022-05-04 02:39:43,710][run.py][line:196][INFO] Best F-Score based on validation:37.98\n",
      "[2022-05-04 02:39:43,710][run.py][line:197][INFO] Best F-Score based on test:39.09\n"
     ]
    }
   ],
   "source": [
    "!python run.py --dataset EmoryNLP --lr 0.00005 --batch_size 32 --epochs 100 --dropout 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(bert_model_dir='', bert_tokenizer_dir='', bert_dim=1024, hidden_dim=300, mlp_layers=2, gnn_layers=3, emb_dim=1024, attn_type='rgcn', no_rel_attn=False, max_sent_len=200, no_cuda=False, dataset_name='DailyDialog', windowp=1, windowf=0, max_grad_norm=5.0, lr=2e-05, dropout=0.3, batch_size=64, epochs=50, tensorboard=False, nodal_att_type=None)\n",
      "Running on GPU\n",
      "building vocab.. \n",
      "building datasets..\n",
      "11118\n",
      "1000\n",
      "1000\n",
      "building model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-04 04:11:06,456][run.py][line:113][INFO] start training on GPU 0!\n",
      "[2022-05-04 04:11:06,457][run.py][line:114][INFO] Namespace(bert_model_dir='', bert_tokenizer_dir='', bert_dim=1024, hidden_dim=300, mlp_layers=2, gnn_layers=3, emb_dim=1024, attn_type='rgcn', no_rel_attn=False, max_sent_len=200, no_cuda=False, dataset_name='DailyDialog', windowp=1, windowf=0, max_grad_norm=5.0, lr=2e-05, dropout=0.3, batch_size=64, epochs=50, tensorboard=False, nodal_att_type=None, cuda=True)\n",
      "E:\\Anaconda\\envs\\gpu2\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2022-05-04 04:13:19,616][run.py][line:154][INFO] Epoch: 1, train_loss: 0.5822, train_acc: 89.55, train_micro_fscore: 64.73, train_macro_fscore: 30.21, valid_loss: 0.3131, valid_acc: 91.06, valid_micro_fscore: 60.02, valid_macro_fscore: 36.41, test_loss: 0.5142, test_acc: 86.01, test_micro_fscore: 56.35, test_macro_fscore: 36.0, time: 65.18 sec\n",
      "[2022-05-04 04:14:11,352][run.py][line:154][INFO] Epoch: 2, train_loss: 0.1682, train_acc: 94.79, train_micro_fscore: 83.01, train_macro_fscore: 55.72, valid_loss: 0.2935, valid_acc: 91.23, valid_micro_fscore: 61.24, valid_macro_fscore: 42.39, test_loss: 0.5074, test_acc: 85.92, test_micro_fscore: 56.86, test_macro_fscore: 39.56, time: 51.73 sec\n",
      "[2022-05-04 04:15:01,800][run.py][line:154][INFO] Epoch: 3, train_loss: 0.1519, train_acc: 95.05, train_micro_fscore: 83.95, train_macro_fscore: 58.48, valid_loss: 0.2901, valid_acc: 91.36, valid_micro_fscore: 61.43, valid_macro_fscore: 42.99, test_loss: 0.5123, test_acc: 86.06, test_micro_fscore: 56.96, test_macro_fscore: 40.53, time: 50.45 sec\n",
      "[2022-05-04 04:15:55,223][run.py][line:154][INFO] Epoch: 4, train_loss: 0.1467, train_acc: 95.05, train_micro_fscore: 84.14, train_macro_fscore: 61.39, valid_loss: 0.29, valid_acc: 91.25, valid_micro_fscore: 61.12, valid_macro_fscore: 42.98, test_loss: 0.511, test_acc: 86.12, test_micro_fscore: 57.48, test_macro_fscore: 43.36, time: 53.42 sec\n",
      "[2022-05-04 04:16:45,458][run.py][line:154][INFO] Epoch: 5, train_loss: 0.1431, train_acc: 95.22, train_micro_fscore: 84.7, train_macro_fscore: 64.54, valid_loss: 0.2828, valid_acc: 91.28, valid_micro_fscore: 61.37, valid_macro_fscore: 43.43, test_loss: 0.4928, test_acc: 86.07, test_micro_fscore: 57.62, test_macro_fscore: 44.97, time: 50.23 sec\n",
      "[2022-05-04 04:17:35,159][run.py][line:154][INFO] Epoch: 6, train_loss: 0.1407, train_acc: 95.28, train_micro_fscore: 84.89, train_macro_fscore: 67.89, valid_loss: 0.2929, valid_acc: 91.09, valid_micro_fscore: 61.28, valid_macro_fscore: 42.53, test_loss: 0.4937, test_acc: 86.09, test_micro_fscore: 58.29, test_macro_fscore: 46.98, time: 49.7 sec\n",
      "[2022-05-04 04:18:24,710][run.py][line:154][INFO] Epoch: 7, train_loss: 0.1385, train_acc: 95.31, train_micro_fscore: 84.99, train_macro_fscore: 68.75, valid_loss: 0.2873, valid_acc: 91.13, valid_micro_fscore: 60.67, valid_macro_fscore: 41.95, test_loss: 0.4989, test_acc: 86.18, test_micro_fscore: 58.16, test_macro_fscore: 46.6, time: 49.55 sec\n",
      "[2022-05-04 04:19:13,683][run.py][line:154][INFO] Epoch: 8, train_loss: 0.1365, train_acc: 95.33, train_micro_fscore: 85.12, train_macro_fscore: 71.28, valid_loss: 0.2866, valid_acc: 91.13, valid_micro_fscore: 61.15, valid_macro_fscore: 45.15, test_loss: 0.491, test_acc: 86.02, test_micro_fscore: 58.23, test_macro_fscore: 46.91, time: 48.97 sec\n",
      "[2022-05-04 04:20:01,545][run.py][line:154][INFO] Epoch: 9, train_loss: 0.1357, train_acc: 95.36, train_micro_fscore: 85.19, train_macro_fscore: 73.48, valid_loss: 0.2835, valid_acc: 91.14, valid_micro_fscore: 60.88, valid_macro_fscore: 44.56, test_loss: 0.4896, test_acc: 86.09, test_micro_fscore: 57.97, test_macro_fscore: 46.29, time: 47.86 sec\n",
      "[2022-05-04 04:20:51,137][run.py][line:154][INFO] Epoch: 10, train_loss: 0.1348, train_acc: 95.42, train_micro_fscore: 85.41, train_macro_fscore: 74.33, valid_loss: 0.2761, valid_acc: 91.19, valid_micro_fscore: 60.84, valid_macro_fscore: 45.07, test_loss: 0.4763, test_acc: 86.19, test_micro_fscore: 58.17, test_macro_fscore: 50.71, time: 49.59 sec\n",
      "[2022-05-04 04:21:38,201][run.py][line:154][INFO] Epoch: 11, train_loss: 0.1334, train_acc: 95.43, train_micro_fscore: 85.45, train_macro_fscore: 75.44, valid_loss: 0.2798, valid_acc: 91.14, valid_micro_fscore: 61.07, valid_macro_fscore: 45.47, test_loss: 0.4855, test_acc: 86.14, test_micro_fscore: 58.26, test_macro_fscore: 52.72, time: 47.06 sec\n",
      "[2022-05-04 04:22:26,942][run.py][line:154][INFO] Epoch: 12, train_loss: 0.1332, train_acc: 95.4, train_micro_fscore: 85.29, train_macro_fscore: 75.04, valid_loss: 0.2807, valid_acc: 91.28, valid_micro_fscore: 61.18, valid_macro_fscore: 45.9, test_loss: 0.4917, test_acc: 86.18, test_micro_fscore: 58.2, test_macro_fscore: 53.36, time: 48.74 sec\n",
      "[2022-05-04 04:23:15,421][run.py][line:154][INFO] Epoch: 13, train_loss: 0.1327, train_acc: 95.47, train_micro_fscore: 85.53, train_macro_fscore: 75.32, valid_loss: 0.2862, valid_acc: 91.06, valid_micro_fscore: 61.33, valid_macro_fscore: 47.77, test_loss: 0.4959, test_acc: 86.05, test_micro_fscore: 58.25, test_macro_fscore: 52.32, time: 48.48 sec\n",
      "[2022-05-04 04:24:04,961][run.py][line:154][INFO] Epoch: 14, train_loss: 0.1311, train_acc: 95.47, train_micro_fscore: 85.57, train_macro_fscore: 76.12, valid_loss: 0.2813, valid_acc: 90.97, valid_micro_fscore: 61.81, valid_macro_fscore: 48.06, test_loss: 0.4808, test_acc: 85.96, test_micro_fscore: 58.87, test_macro_fscore: 52.67, time: 49.54 sec\n",
      "[2022-05-04 04:24:54,698][run.py][line:154][INFO] Epoch: 15, train_loss: 0.131, train_acc: 95.51, train_micro_fscore: 85.67, train_macro_fscore: 76.46, valid_loss: 0.2749, valid_acc: 91.46, valid_micro_fscore: 60.72, valid_macro_fscore: 46.96, test_loss: 0.4937, test_acc: 86.27, test_micro_fscore: 56.96, test_macro_fscore: 52.3, time: 49.74 sec\n",
      "[2022-05-04 04:25:43,296][run.py][line:154][INFO] Epoch: 16, train_loss: 0.1306, train_acc: 95.51, train_micro_fscore: 85.6, train_macro_fscore: 76.42, valid_loss: 0.2844, valid_acc: 91.04, valid_micro_fscore: 61.53, valid_macro_fscore: 46.35, test_loss: 0.496, test_acc: 86.05, test_micro_fscore: 58.73, test_macro_fscore: 53.12, time: 48.6 sec\n",
      "[2022-05-04 04:26:34,345][run.py][line:154][INFO] Epoch: 17, train_loss: 0.1297, train_acc: 95.49, train_micro_fscore: 85.67, train_macro_fscore: 76.26, valid_loss: 0.2821, valid_acc: 91.31, valid_micro_fscore: 61.51, valid_macro_fscore: 47.81, test_loss: 0.4954, test_acc: 86.18, test_micro_fscore: 58.21, test_macro_fscore: 52.58, time: 51.05 sec\n",
      "[2022-05-04 04:27:21,934][run.py][line:154][INFO] Epoch: 18, train_loss: 0.1296, train_acc: 95.5, train_micro_fscore: 85.63, train_macro_fscore: 76.36, valid_loss: 0.2866, valid_acc: 90.94, valid_micro_fscore: 61.89, valid_macro_fscore: 48.36, test_loss: 0.4802, test_acc: 85.9, test_micro_fscore: 58.95, test_macro_fscore: 53.32, time: 47.59 sec\n",
      "[2022-05-04 04:28:13,262][run.py][line:154][INFO] Epoch: 19, train_loss: 0.1288, train_acc: 95.56, train_micro_fscore: 85.8, train_macro_fscore: 76.22, valid_loss: 0.2781, valid_acc: 91.44, valid_micro_fscore: 61.18, valid_macro_fscore: 46.85, test_loss: 0.4861, test_acc: 86.29, test_micro_fscore: 57.62, test_macro_fscore: 51.92, time: 51.33 sec\n",
      "[2022-05-04 04:29:03,197][run.py][line:154][INFO] Epoch: 20, train_loss: 0.1283, train_acc: 95.55, train_micro_fscore: 85.8, train_macro_fscore: 77.01, valid_loss: 0.2814, valid_acc: 91.44, valid_micro_fscore: 61.44, valid_macro_fscore: 47.68, test_loss: 0.4956, test_acc: 86.4, test_micro_fscore: 58.16, test_macro_fscore: 53.88, time: 49.93 sec\n",
      "[2022-05-04 04:29:53,116][run.py][line:154][INFO] Epoch: 21, train_loss: 0.1271, train_acc: 95.59, train_micro_fscore: 85.88, train_macro_fscore: 76.5, valid_loss: 0.291, valid_acc: 90.89, valid_micro_fscore: 61.74, valid_macro_fscore: 48.23, test_loss: 0.4987, test_acc: 86.01, test_micro_fscore: 59.17, test_macro_fscore: 53.39, time: 49.92 sec\n",
      "[2022-05-04 04:30:44,981][run.py][line:154][INFO] Epoch: 22, train_loss: 0.1274, train_acc: 95.6, train_micro_fscore: 85.97, train_macro_fscore: 77.07, valid_loss: 0.2825, valid_acc: 91.34, valid_micro_fscore: 61.54, valid_macro_fscore: 47.81, test_loss: 0.4959, test_acc: 86.29, test_micro_fscore: 58.38, test_macro_fscore: 52.81, time: 51.87 sec\n",
      "[2022-05-04 04:31:35,709][run.py][line:154][INFO] Epoch: 23, train_loss: 0.1272, train_acc: 95.6, train_micro_fscore: 85.97, train_macro_fscore: 76.7, valid_loss: 0.2909, valid_acc: 91.28, valid_micro_fscore: 61.45, valid_macro_fscore: 48.06, test_loss: 0.5108, test_acc: 86.14, test_micro_fscore: 58.37, test_macro_fscore: 53.5, time: 50.73 sec\n",
      "[2022-05-04 04:32:26,383][run.py][line:154][INFO] Epoch: 24, train_loss: 0.1264, train_acc: 95.61, train_micro_fscore: 85.97, train_macro_fscore: 77.14, valid_loss: 0.2874, valid_acc: 91.15, valid_micro_fscore: 61.25, valid_macro_fscore: 47.82, test_loss: 0.4993, test_acc: 86.1, test_micro_fscore: 58.29, test_macro_fscore: 52.98, time: 50.67 sec\n",
      "[2022-05-04 04:33:16,314][run.py][line:154][INFO] Epoch: 25, train_loss: 0.1256, train_acc: 95.63, train_micro_fscore: 86.05, train_macro_fscore: 77.16, valid_loss: 0.2835, valid_acc: 91.18, valid_micro_fscore: 61.43, valid_macro_fscore: 47.37, test_loss: 0.4915, test_acc: 86.19, test_micro_fscore: 58.55, test_macro_fscore: 52.82, time: 49.93 sec\n",
      "[2022-05-04 04:34:05,993][run.py][line:154][INFO] Epoch: 26, train_loss: 0.1261, train_acc: 95.62, train_micro_fscore: 86.05, train_macro_fscore: 77.66, valid_loss: 0.2908, valid_acc: 91.11, valid_micro_fscore: 61.21, valid_macro_fscore: 47.48, test_loss: 0.4993, test_acc: 86.03, test_micro_fscore: 58.26, test_macro_fscore: 53.02, time: 49.68 sec\n",
      "[2022-05-04 04:34:54,888][run.py][line:154][INFO] Epoch: 27, train_loss: 0.125, train_acc: 95.65, train_micro_fscore: 86.07, train_macro_fscore: 77.42, valid_loss: 0.2833, valid_acc: 91.19, valid_micro_fscore: 61.09, valid_macro_fscore: 47.47, test_loss: 0.4947, test_acc: 86.32, test_micro_fscore: 58.62, test_macro_fscore: 53.38, time: 48.9 sec\n",
      "[2022-05-04 04:35:45,213][run.py][line:154][INFO] Epoch: 28, train_loss: 0.1255, train_acc: 95.63, train_micro_fscore: 86.05, train_macro_fscore: 77.7, valid_loss: 0.2811, valid_acc: 91.19, valid_micro_fscore: 61.55, valid_macro_fscore: 47.56, test_loss: 0.486, test_acc: 86.07, test_micro_fscore: 58.32, test_macro_fscore: 53.45, time: 50.32 sec\n",
      "[2022-05-04 04:36:34,988][run.py][line:154][INFO] Epoch: 29, train_loss: 0.1245, train_acc: 95.61, train_micro_fscore: 86.0, train_macro_fscore: 77.35, valid_loss: 0.2821, valid_acc: 91.09, valid_micro_fscore: 61.73, valid_macro_fscore: 48.16, test_loss: 0.4918, test_acc: 85.99, test_micro_fscore: 58.43, test_macro_fscore: 53.21, time: 49.77 sec\n",
      "[2022-05-04 04:37:24,277][run.py][line:154][INFO] Epoch: 30, train_loss: 0.1244, train_acc: 95.69, train_micro_fscore: 86.22, train_macro_fscore: 77.64, valid_loss: 0.2806, valid_acc: 91.2, valid_micro_fscore: 61.65, valid_macro_fscore: 48.18, test_loss: 0.4962, test_acc: 86.1, test_micro_fscore: 58.28, test_macro_fscore: 52.68, time: 49.29 sec\n",
      "[2022-05-04 04:38:12,739][run.py][line:154][INFO] Epoch: 31, train_loss: 0.124, train_acc: 95.68, train_micro_fscore: 86.28, train_macro_fscore: 78.02, valid_loss: 0.2875, valid_acc: 91.49, valid_micro_fscore: 60.83, valid_macro_fscore: 46.9, test_loss: 0.5256, test_acc: 86.47, test_micro_fscore: 57.33, test_macro_fscore: 52.64, time: 48.46 sec\n",
      "[2022-05-04 04:39:03,177][run.py][line:154][INFO] Epoch: 32, train_loss: 0.1235, train_acc: 95.69, train_micro_fscore: 86.21, train_macro_fscore: 77.81, valid_loss: 0.284, valid_acc: 91.26, valid_micro_fscore: 60.97, valid_macro_fscore: 46.66, test_loss: 0.4975, test_acc: 86.36, test_micro_fscore: 58.25, test_macro_fscore: 52.91, time: 50.44 sec\n",
      "[2022-05-04 04:39:55,078][run.py][line:154][INFO] Epoch: 33, train_loss: 0.1235, train_acc: 95.68, train_micro_fscore: 86.23, train_macro_fscore: 77.96, valid_loss: 0.288, valid_acc: 91.2, valid_micro_fscore: 61.33, valid_macro_fscore: 47.6, test_loss: 0.503, test_acc: 86.14, test_micro_fscore: 58.24, test_macro_fscore: 53.1, time: 51.9 sec\n",
      "[2022-05-04 04:40:45,682][run.py][line:154][INFO] Epoch: 34, train_loss: 0.1223, train_acc: 95.69, train_micro_fscore: 86.25, train_macro_fscore: 77.61, valid_loss: 0.2817, valid_acc: 91.34, valid_micro_fscore: 61.52, valid_macro_fscore: 48.36, test_loss: 0.5019, test_acc: 86.14, test_micro_fscore: 57.87, test_macro_fscore: 53.26, time: 50.6 sec\n",
      "[2022-05-04 04:41:35,754][run.py][line:154][INFO] Epoch: 35, train_loss: 0.1222, train_acc: 95.72, train_micro_fscore: 86.37, train_macro_fscore: 78.28, valid_loss: 0.2897, valid_acc: 91.46, valid_micro_fscore: 60.77, valid_macro_fscore: 47.2, test_loss: 0.5216, test_acc: 86.5, test_micro_fscore: 57.37, test_macro_fscore: 52.72, time: 50.07 sec\n",
      "[2022-05-04 04:42:27,605][run.py][line:154][INFO] Epoch: 36, train_loss: 0.1225, train_acc: 95.72, train_micro_fscore: 86.33, train_macro_fscore: 78.28, valid_loss: 0.285, valid_acc: 91.4, valid_micro_fscore: 61.09, valid_macro_fscore: 47.61, test_loss: 0.5193, test_acc: 86.33, test_micro_fscore: 57.45, test_macro_fscore: 52.81, time: 51.85 sec\n",
      "[2022-05-04 04:43:15,515][run.py][line:154][INFO] Epoch: 37, train_loss: 0.1224, train_acc: 95.68, train_micro_fscore: 86.23, train_macro_fscore: 77.73, valid_loss: 0.2808, valid_acc: 91.34, valid_micro_fscore: 61.15, valid_macro_fscore: 47.51, test_loss: 0.4992, test_acc: 86.27, test_micro_fscore: 57.9, test_macro_fscore: 52.36, time: 47.91 sec\n",
      "[2022-05-04 04:44:03,103][run.py][line:154][INFO] Epoch: 38, train_loss: 0.1214, train_acc: 95.75, train_micro_fscore: 86.41, train_macro_fscore: 78.38, valid_loss: 0.2896, valid_acc: 91.35, valid_micro_fscore: 61.59, valid_macro_fscore: 48.21, test_loss: 0.5127, test_acc: 86.29, test_micro_fscore: 58.34, test_macro_fscore: 52.7, time: 47.59 sec\n",
      "[2022-05-04 04:44:50,563][run.py][line:154][INFO] Epoch: 39, train_loss: 0.1223, train_acc: 95.72, train_micro_fscore: 86.38, train_macro_fscore: 78.33, valid_loss: 0.2832, valid_acc: 91.34, valid_micro_fscore: 61.73, valid_macro_fscore: 48.28, test_loss: 0.4988, test_acc: 86.14, test_micro_fscore: 57.91, test_macro_fscore: 52.87, time: 47.46 sec\n",
      "[2022-05-04 04:45:40,977][run.py][line:154][INFO] Epoch: 40, train_loss: 0.1215, train_acc: 95.7, train_micro_fscore: 86.29, train_macro_fscore: 78.02, valid_loss: 0.2962, valid_acc: 91.03, valid_micro_fscore: 61.55, valid_macro_fscore: 48.47, test_loss: 0.5106, test_acc: 85.97, test_micro_fscore: 58.62, test_macro_fscore: 53.29, time: 50.41 sec\n",
      "[2022-05-04 04:46:30,458][run.py][line:154][INFO] Epoch: 41, train_loss: 0.1208, train_acc: 95.71, train_micro_fscore: 86.28, train_macro_fscore: 78.46, valid_loss: 0.2857, valid_acc: 91.39, valid_micro_fscore: 60.85, valid_macro_fscore: 47.52, test_loss: 0.517, test_acc: 86.34, test_micro_fscore: 57.36, test_macro_fscore: 52.68, time: 49.48 sec\n",
      "[2022-05-04 04:47:18,013][run.py][line:154][INFO] Epoch: 42, train_loss: 0.1209, train_acc: 95.8, train_micro_fscore: 86.6, train_macro_fscore: 78.44, valid_loss: 0.2869, valid_acc: 91.21, valid_micro_fscore: 61.6, valid_macro_fscore: 48.32, test_loss: 0.5041, test_acc: 86.14, test_micro_fscore: 58.21, test_macro_fscore: 52.83, time: 47.55 sec\n",
      "[2022-05-04 04:48:06,323][run.py][line:154][INFO] Epoch: 43, train_loss: 0.1203, train_acc: 95.72, train_micro_fscore: 86.35, train_macro_fscore: 78.08, valid_loss: 0.2882, valid_acc: 90.94, valid_micro_fscore: 61.24, valid_macro_fscore: 48.81, test_loss: 0.502, test_acc: 85.85, test_micro_fscore: 58.23, test_macro_fscore: 52.76, time: 48.31 sec\n",
      "[2022-05-04 04:48:57,749][run.py][line:154][INFO] Epoch: 44, train_loss: 0.1204, train_acc: 95.75, train_micro_fscore: 86.46, train_macro_fscore: 78.15, valid_loss: 0.2993, valid_acc: 91.1, valid_micro_fscore: 61.68, valid_macro_fscore: 47.95, test_loss: 0.5176, test_acc: 86.24, test_micro_fscore: 58.74, test_macro_fscore: 52.91, time: 51.43 sec\n",
      "[2022-05-04 04:49:45,213][run.py][line:154][INFO] Epoch: 45, train_loss: 0.1202, train_acc: 95.75, train_micro_fscore: 86.39, train_macro_fscore: 77.81, valid_loss: 0.2875, valid_acc: 91.35, valid_micro_fscore: 61.0, valid_macro_fscore: 48.13, test_loss: 0.5175, test_acc: 86.27, test_micro_fscore: 57.36, test_macro_fscore: 52.73, time: 47.46 sec\n",
      "[2022-05-04 04:50:32,272][run.py][line:154][INFO] Epoch: 46, train_loss: 0.1194, train_acc: 95.78, train_micro_fscore: 86.58, train_macro_fscore: 78.51, valid_loss: 0.2835, valid_acc: 91.24, valid_micro_fscore: 61.68, valid_macro_fscore: 48.34, test_loss: 0.5052, test_acc: 86.09, test_micro_fscore: 57.93, test_macro_fscore: 52.84, time: 47.06 sec\n",
      "[2022-05-04 04:51:19,365][run.py][line:154][INFO] Epoch: 47, train_loss: 0.1194, train_acc: 95.77, train_micro_fscore: 86.51, train_macro_fscore: 78.69, valid_loss: 0.2875, valid_acc: 91.36, valid_micro_fscore: 61.5, valid_macro_fscore: 48.63, test_loss: 0.5122, test_acc: 85.99, test_micro_fscore: 57.18, test_macro_fscore: 52.63, time: 47.09 sec\n",
      "[2022-05-04 04:52:10,618][run.py][line:154][INFO] Epoch: 48, train_loss: 0.1193, train_acc: 95.8, train_micro_fscore: 86.62, train_macro_fscore: 78.84, valid_loss: 0.2818, valid_acc: 91.35, valid_micro_fscore: 61.01, valid_macro_fscore: 47.87, test_loss: 0.5008, test_acc: 86.21, test_micro_fscore: 57.45, test_macro_fscore: 52.62, time: 51.25 sec\n",
      "[2022-05-04 04:53:02,117][run.py][line:154][INFO] Epoch: 49, train_loss: 0.1182, train_acc: 95.78, train_micro_fscore: 86.58, train_macro_fscore: 79.16, valid_loss: 0.2835, valid_acc: 91.4, valid_micro_fscore: 61.76, valid_macro_fscore: 48.34, test_loss: 0.5115, test_acc: 86.25, test_micro_fscore: 57.94, test_macro_fscore: 53.17, time: 51.5 sec\n",
      "[2022-05-04 04:53:53,216][run.py][line:154][INFO] Epoch: 50, train_loss: 0.1178, train_acc: 95.81, train_micro_fscore: 86.63, train_macro_fscore: 78.35, valid_loss: 0.2912, valid_acc: 91.37, valid_micro_fscore: 61.19, valid_macro_fscore: 47.88, test_loss: 0.525, test_acc: 86.4, test_micro_fscore: 57.76, test_macro_fscore: 52.5, time: 51.1 sec\n",
      "[2022-05-04 04:53:53,216][run.py][line:180][INFO] finish training!\n",
      "[2022-05-04 04:53:53,216][run.py][line:192][INFO] Best micro/macro F-Score based on validation:58.95/53.32\n",
      "[2022-05-04 04:53:53,216][run.py][line:194][INFO] Best micro/macro F-Score based on test:59.17/53.39\n"
     ]
    }
   ],
   "source": [
    "!python run.py --dataset DailyDialog --gnn_layers 3 --lr 0.00002 --batch_size 64 --epochs 50 --dropout 0.3"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67f69efe151d0dcdef6fdfc0180de1a7cdfe8a3bf7735cd9d7e5945c5774d967"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('gpu2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
